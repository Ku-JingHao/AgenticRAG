{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7786feb",
   "metadata": {},
   "source": [
    "# Agentic RAG System Implementation\n",
    "\n",
    "This notebook implements a complete Agentic RAG (Retrieval-Augmented Generation) system that goes beyond traditional RAG by incorporating intelligent agents that can reason, plan, and make decisions about how to retrieve and use information.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Dependencies](#setup)  \n",
    "2. [Traditional RAG vs Agentic RAG](#comparison)  \n",
    "3. [Document Processing and Indexing](#processing)  \n",
    "4. [Multi-Modal Indexing System](#optimization)  \n",
    "5. [Agent Implementation](#agents)  \n",
    "6. [Orchestration Implementation](#orchestration)  \n",
    "7. [Testing Framework](#testing1)  \n",
    "8. [Performance Analysis](#analysis)\n",
    "9. [Comprehensive Test Framework](#testing2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51885896",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies <a id=\"setup\"></a>\n",
    "\n",
    "First, let's install and import all necessary libraries for our Agentic RAG system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f75ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.31-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.2)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Collecting openai<2.0.0,>=1.104.2 (from langchain-openai)\n",
      "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Collecting packaging>=23.2 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.8.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.104.2->langchain-openai)\n",
      "  Downloading jiter-0.11.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.10.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.104.2->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.1/1.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.2/1.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.3/1.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.4/1.0 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.6/1.0 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.7/1.0 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.8/1.0 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 0.9/1.0 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.0/1.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.5 MB 2.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.6/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.7/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.7/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.9/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.1/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.5/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.8/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.9/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.0/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.5/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/75.0 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 41.0/75.0 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 75.0/75.0 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "   ---------------------------------------- 0.0/447.5 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 41.0/447.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 143.4/447.5 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 235.5/447.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 327.7/447.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  440.3/447.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 447.5/447.5 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.31-py3-none-any.whl (386 kB)\n",
      "   ---------------------------------------- 0.0/386.3 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 122.9/386.3 kB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 225.3/386.3 kB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 317.4/386.3 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 386.3/386.3 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "   ---------------------------------------- 0.0/948.6 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 61.4/948.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 143.4/948.6 kB 1.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 235.5/948.6 kB 1.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 337.9/948.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 419.8/948.6 kB 1.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 481.3/948.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 563.2/948.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 665.6/948.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 737.3/948.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 809.0/948.6 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 890.9/948.6 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 948.6/948.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "   ---------------------------------------- 0.0/48.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 48.6/48.6 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.7 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 64.7/64.7 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.4 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 71.7/884.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 153.6/884.4 kB 1.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 256.0/884.4 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 348.2/884.4 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 471.0/884.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 563.2/884.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 675.8/884.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 778.2/884.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  880.6/884.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 884.4/884.4 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading jiter-0.11.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "   ---------------------------------------- 0.0/204.3 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 71.7/204.3 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 174.1/204.3 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 204.3/204.3 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 66.5/66.5 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl (506 kB)\n",
      "   ---------------------------------------- 0.0/506.2 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 71.7/506.2 kB 4.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 194.6/506.2 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 286.7/506.2 kB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 368.6/506.2 kB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 471.0/506.2 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 506.2/506.2 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: zstandard, typing-inspection, typing-inspect, requests, packaging, jsonpatch, jiter, httpx-sse, tiktoken, marshmallow, pydantic-settings, openai, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.19.0\n",
      "    Uninstalling zstandard-0.19.0:\n",
      "      Successfully uninstalled zstandard-0.19.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 jiter-0.11.0 jsonpatch-1.33 langchain-0.3.27 langchain-community-0.3.30 langchain-core-0.3.76 langchain-openai-0.3.33 langchain-text-splitters-0.3.11 langsmith-0.4.31 marshmallow-3.26.1 openai-1.109.1 packaging-25.0 pydantic-settings-2.11.0 requests-2.32.5 tiktoken-0.11.0 typing-inspect-0.9.0 typing-inspection-0.4.2 zstandard-0.25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "alpaca-trade-api 3.2.0 requires websockets<11,>=9.0, but you have websockets 15.0.1 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.5 which is incompatible.\n",
      "darts 0.32.0 requires scikit-learn<1.6.0,>=1.0.1, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "streamlit 1.44.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.19.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.1.0-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (2.10.3)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.23.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB 1.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (1.68.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (13.3.5)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from chromadb) (4.19.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.1/40.1 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\omen\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\omen\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\omen\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\omen\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\omen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\omen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.26.20)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\omen\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\omen\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\omen\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Downloading chromadb-1.1.0-cp39-abi3-win_amd64.whl (19.8 MB)\n",
      "   ---------------------------------------- 0.0/19.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/19.8 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.1/19.8 MB 1.1 MB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.2/19.8 MB 1.7 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.3/19.8 MB 1.6 MB/s eta 0:00:13\n",
      "    --------------------------------------- 0.4/19.8 MB 1.7 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/19.8 MB 1.7 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.6/19.8 MB 1.8 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.6/19.8 MB 1.8 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.8/19.8 MB 1.9 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.9/19.8 MB 2.0 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 1.0/19.8 MB 2.0 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.0/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.1/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.2/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.4/19.8 MB 2.0 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.5/19.8 MB 2.0 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.6/19.8 MB 2.0 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.7/19.8 MB 2.0 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.7/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.8/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.9/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.9/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.0/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.1/19.8 MB 1.8 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.2/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.3/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.4/19.8 MB 1.9 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.5/19.8 MB 1.9 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.6/19.8 MB 1.9 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.7/19.8 MB 1.9 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.8/19.8 MB 1.9 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.9/19.8 MB 1.9 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 3.0/19.8 MB 1.9 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 3.1/19.8 MB 2.0 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 3.2/19.8 MB 2.0 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 3.3/19.8 MB 2.0 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 3.4/19.8 MB 2.0 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.5/19.8 MB 2.0 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.6/19.8 MB 2.0 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.7/19.8 MB 2.0 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.8/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 3.9/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 4.0/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 4.1/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 4.2/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 4.3/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 4.3/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 4.4/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 4.5/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 4.6/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 4.7/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 4.8/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 4.9/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.0/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.1/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.2/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.3/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.3/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.4/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 5.5/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 5.6/19.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 5.7/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 5.9/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 6.0/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 6.1/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 6.1/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 6.2/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 6.3/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 6.3/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 6.4/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.5/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.5/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.6/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.7/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.7/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.8/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.9/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 7.0/19.8 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 7.0/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 7.1/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 7.2/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 7.2/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 7.3/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 7.4/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 7.4/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 7.5/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 7.6/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 7.7/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 7.7/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 7.8/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 8.0/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 8.0/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 8.1/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 8.2/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 8.3/19.8 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 8.4/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 8.5/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 8.6/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 8.7/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 8.8/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 8.9/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 8.9/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 9.0/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 9.1/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 9.2/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 9.3/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 9.4/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 9.5/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 9.6/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 9.7/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 9.8/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 9.9/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 10.0/19.8 MB 1.9 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 10.1/19.8 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 10.2/19.8 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 10.3/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.4/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.5/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.6/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.7/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.8/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 10.9/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 11.0/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 11.1/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 11.2/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 11.3/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 11.4/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 11.4/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 11.5/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 11.6/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 11.7/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 11.8/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 11.9/19.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 12.0/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 12.1/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 12.2/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 12.2/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 12.3/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 12.4/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 12.5/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 12.6/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 12.7/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 12.8/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 12.9/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 13.0/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 13.1/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 13.2/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 13.3/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 13.4/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 13.5/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 13.6/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 13.7/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 13.8/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.9/19.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 14.0/19.8 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 14.0/19.8 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 14.1/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 14.1/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 14.2/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 14.3/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 14.3/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 14.4/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 14.5/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 14.6/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 14.7/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 14.8/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 15.0/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 15.0/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 15.1/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 15.2/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 15.3/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 15.4/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 15.5/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 15.6/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 15.7/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 15.8/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 15.9/19.8 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 16.0/19.8 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 16.1/19.8 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 16.2/19.8 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 16.3/19.8 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 16.4/19.8 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 16.5/19.8 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 16.5/19.8 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 16.6/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 16.7/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 16.8/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 17.0/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 17.0/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 17.2/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 17.2/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.3/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.4/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.5/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.6/19.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.7/19.8 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 17.9/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.9/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 18.0/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 18.1/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 18.2/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.4/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.5/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.6/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.6/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.7/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.8/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.9/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.0/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.1/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.2/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.3/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.4/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.5/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.6/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.7/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.8/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.8/19.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.8/19.8 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "   ---------------------------------------- 0.0/486.6 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 112.6/486.6 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 204.8/486.6 kB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 307.2/486.6 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 419.8/486.6 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 486.6/486.6 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading bcrypt-5.0.0-cp39-abi3-win_amd64.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.9 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 112.6/150.9 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.9/150.9 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.3 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 92.2/564.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 184.3/564.3 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 276.5/564.3 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 317.4/564.3 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 358.4/564.3 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 399.4/564.3 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 450.6/564.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 501.8/564.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 532.5/564.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 564.3/564.3 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.0 MB 1.4 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.1/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/2.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/2.0 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.6/2.0 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.7/2.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.2/2.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/2.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.4/2.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.5/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading mmh3-5.2.0-cp311-cp311-win_amd64.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.5/41.5 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading onnxruntime-1.23.0-cp311-cp311-win_amd64.whl (13.4 MB)\n",
      "   ---------------------------------------- 0.0/13.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.4 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 0.1/13.4 MB 1.8 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/13.4 MB 1.8 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.3/13.4 MB 1.8 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/13.4 MB 1.8 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.5/13.4 MB 1.8 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.6/13.4 MB 1.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.7/13.4 MB 1.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/13.4 MB 1.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.9/13.4 MB 2.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.0/13.4 MB 2.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.1/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.2/13.4 MB 2.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.3/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.4/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.5/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.6/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.7/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.8/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.9/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.0/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.1/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.2/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.3/13.4 MB 2.1 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.4/13.4 MB 2.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.5/13.4 MB 2.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.6/13.4 MB 2.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.7/13.4 MB 2.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.8/13.4 MB 2.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.9/13.4 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.0/13.4 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.1/13.4 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.2/13.4 MB 2.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.3/13.4 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.3/13.4 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.4/13.4 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.4/13.4 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.5/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.5/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.6/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.7/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.8/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.8/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.9/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.0/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.1/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.1/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.2/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.3/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.4/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.5/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.6/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.7/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.8/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.9/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.9/13.4 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.0/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.0/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.1/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.2/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.2/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.3/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.4/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.4/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.5/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.6/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.6/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.7/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.8/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.9/13.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 6.0/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.1/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.2/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.3/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.4/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.5/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.6/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.6/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.7/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.9/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.0/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.1/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.2/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.2/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.2/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.3/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.4/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.5/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.6/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.7/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.7/13.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.8/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.9/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.0/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.1/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.2/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.3/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.4/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.5/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.6/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.7/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.8/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.9/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.9/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.0/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.1/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.2/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.3/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.5/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.5/13.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.6/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 9.7/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.7/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.8/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.9/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.9/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.0/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.1/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.2/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.3/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.4/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.5/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.6/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.7/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.8/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.9/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.0/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.1/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.2/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.2/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.3/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.4/13.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.5/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.7/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.8/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.8/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.0/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.0/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.1/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.2/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.4/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.5/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.5/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.7/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.8/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.9/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.0/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.3/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.4/13.4 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.7 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 41.0/65.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 65.7/65.7 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
      "   ---------------------------------------- 0.0/72.5 kB ? eta -:--:--\n",
      "   ---------------------------------------  71.7/72.5 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.5/72.5 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
      "   ---------------------------------------- 0.0/131.9 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 71.7/131.9 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 131.9/131.9 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
      "   ---------------------------------------- 0.0/208.0 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 102.4/208.0 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  204.8/208.0 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 208.0/208.0 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 105.4/105.4 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading pybase64-1.4.2-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.7 MB 2.6 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.1/2.7 MB 2.1 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.7 MB 1.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.7 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.7 MB 1.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.7 MB 2.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.7 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.7/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.9/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.0/2.7 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.1/2.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.2/2.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.3/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.4/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.5/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.7/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.9/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.0/2.7 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.1/2.7 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.2/2.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.3/2.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.5/2.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.6 MB 2.6 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/11.6 MB 2.4 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.3/11.6 MB 2.0 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/11.6 MB 2.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/11.6 MB 2.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/11.6 MB 2.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.7/11.6 MB 2.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/11.6 MB 2.1 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/11.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.9/11.6 MB 1.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.0/11.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.1/11.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.1/11.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.4/11.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.5/11.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.7/11.6 MB 2.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.8/11.6 MB 2.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.9/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.9/11.6 MB 2.1 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.0/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.2/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.3/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.3/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.4/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.5/11.6 MB 1.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.5/11.6 MB 1.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.6/11.6 MB 1.9 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.7/11.6 MB 1.9 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.8/11.6 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.9/11.6 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.0/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.2/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.3/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.5/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.6/11.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.6/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.8/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.9/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.0/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.1/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.1/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.2/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.3/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.4/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.5/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.6/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.8/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.8/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.9/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.0/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.1/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.3/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.4/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.5/11.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.6/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.7/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.8/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.9/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.0/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.1/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.2/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.3/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.4/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.5/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.7/11.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.9/11.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.0/11.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.1/11.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.2/11.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.2/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.3/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.4/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.5/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.5/11.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.6/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.7/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.8/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.9/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.0/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.1/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.2/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.3/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.4/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.5/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.8/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.9/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.0/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.1/11.6 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.2/11.6 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.3/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.4/11.6 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.5/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.6/11.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.8/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.9/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.9/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.1/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.1/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.3/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.4/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.6/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.7/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.8/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.9/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.2/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.7/46.7 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
      "   ---------------------------------------- 0.0/88.6 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 81.9/88.6 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 88.6/88.6 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.8 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 71.7/434.8 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 184.3/434.8 kB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 297.0/434.8 kB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 399.4/434.8 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 434.8/434.8 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "   ---------------------------------------- 0.0/320.2 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 71.7/320.2 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 174.1/320.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 204.8/320.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  317.4/320.2 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 320.2/320.2 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 41.0/86.8 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 86.8/86.8 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "   ---------------------------------------- 0.0/160.1 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 61.4/160.1 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 122.9/160.1 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 160.1/160.1 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "   ---------------------------------------- 0.0/83.2 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 61.4/83.2 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 83.2/83.2 kB 1.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=5c1e3372d592f284b902caca59255490e6672abf10c2376f6fd6db02bab3d84e\n",
      "  Stored in directory: c:\\users\\omen\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, tenacity, shellingham, safetensors, pyreadline3, pyproject_hooks, pybase64, protobuf, oauthlib, mmh3, importlib-resources, httptools, bcrypt, backoff, requests-oauthlib, posthog, opentelemetry-proto, opentelemetry-api, humanfriendly, huggingface-hub, build, typer, tokenizers, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, kubernetes, coloredlogs, transformers, opentelemetry-sdk, onnxruntime, sentence-transformers, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "Successfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 chromadb-1.1.0 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 huggingface-hub-0.35.3 humanfriendly-10.0 importlib-resources-6.5.2 kubernetes-34.1.0 mmh3-5.2.0 oauthlib-3.3.1 onnxruntime-1.23.0 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 posthog-5.4.0 protobuf-5.29.2 pybase64-1.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 safetensors-0.6.2 sentence-transformers-5.1.1 shellingham-1.5.4 tenacity-9.1.2 tokenizers-0.22.1 transformers-4.56.2 typer-0.19.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.44.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.19.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\omen\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Requirement already satisfied: streamlit in c:\\users\\omen\\anaconda3\\lib\\site-packages (1.44.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\omen\\anaconda3\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (4.2.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Collecting packaging<25,>=20 (from streamlit)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (2.1.4)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (5.29.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\omen\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\omen\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: packaging\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed packaging-24.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.19.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\omen\\anaconda3\\lib\\site-packages (1.109.1)\n",
      "Collecting anthropic\n",
      "  Downloading anthropic-0.69.0-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading anthropic-0.69.0-py3-none-any.whl (337 kB)\n",
      "   ---------------------------------------- 0.0/337.3 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 30.7/337.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 102.4/337.3 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 225.3/337.3 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 307.2/337.3 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 337.3/337.3 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: docstring-parser, anthropic\n",
      "Successfully installed anthropic-0.69.0 docstring-parser-0.17.0\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp311-cp311-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\omen\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.12.0-cp311-cp311-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.2 MB 640.0 kB/s eta 0:00:29\n",
      "   ---------------------------------------- 0.0/18.2 MB 435.7 kB/s eta 0:00:42\n",
      "   ---------------------------------------- 0.1/18.2 MB 837.8 kB/s eta 0:00:22\n",
      "   ---------------------------------------- 0.2/18.2 MB 1.1 MB/s eta 0:00:17\n",
      "    --------------------------------------- 0.3/18.2 MB 1.4 MB/s eta 0:00:14\n",
      "    --------------------------------------- 0.4/18.2 MB 1.5 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/18.2 MB 1.6 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.5/18.2 MB 1.6 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.6/18.2 MB 1.5 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.6/18.2 MB 1.4 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.6/18.2 MB 1.4 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.7/18.2 MB 1.3 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.7/18.2 MB 1.3 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.8/18.2 MB 1.3 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.8/18.2 MB 1.3 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.9/18.2 MB 1.3 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.9/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.0/18.2 MB 1.3 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.1/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.1/18.2 MB 1.3 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.2/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.2/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.3/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.3/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.4/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.4/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.5/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.5/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.6/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.6/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.7/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.7/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.8/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.8/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.9/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.9/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.9/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 2.0/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 2.0/18.2 MB 1.2 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 2.1/18.2 MB 1.1 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 2.1/18.2 MB 1.1 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 2.2/18.2 MB 1.1 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 2.2/18.2 MB 1.1 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 2.3/18.2 MB 1.1 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 2.3/18.2 MB 1.1 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 2.4/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 2.4/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 2.5/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 2.6/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 2.6/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 2.7/18.2 MB 1.2 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 2.8/18.2 MB 1.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 2.9/18.2 MB 1.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 2.9/18.2 MB 1.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 3.0/18.2 MB 1.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 3.1/18.2 MB 1.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 3.2/18.2 MB 1.2 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 3.2/18.2 MB 1.2 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 3.3/18.2 MB 1.2 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 3.4/18.2 MB 1.2 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 3.4/18.2 MB 1.2 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 3.5/18.2 MB 1.3 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 3.6/18.2 MB 1.3 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 3.7/18.2 MB 1.3 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 3.7/18.2 MB 1.3 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 3.8/18.2 MB 1.3 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 3.9/18.2 MB 1.3 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 4.0/18.2 MB 1.3 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 4.1/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 4.1/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 4.2/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 4.3/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 4.3/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 4.4/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 4.5/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 4.6/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 4.6/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 4.7/18.2 MB 1.3 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 4.8/18.2 MB 1.3 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 4.9/18.2 MB 1.3 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 4.9/18.2 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.0/18.2 MB 1.4 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.1/18.2 MB 1.4 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.2/18.2 MB 1.4 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.3/18.2 MB 1.4 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.4/18.2 MB 1.4 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 5.5/18.2 MB 1.4 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 5.6/18.2 MB 1.4 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 5.7/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 5.7/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 5.8/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 5.9/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.0/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.0/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.0/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.1/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.1/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.2/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.2/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.3/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.3/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.3/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.3/18.2 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 6.3/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.4/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.4/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.5/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.5/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.5/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.6/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.6/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.7/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.7/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.7/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 6.8/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 6.9/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 6.9/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 7.0/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 7.0/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 7.1/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 7.1/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 7.1/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 7.1/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 7.2/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 7.2/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 7.3/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 7.4/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 7.4/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 7.5/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 7.6/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 7.6/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 7.7/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 7.8/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 7.8/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 7.9/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 8.0/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 8.0/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 8.1/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 8.2/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 8.2/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 8.3/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 8.4/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 8.4/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 8.5/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 8.6/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 8.7/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 8.7/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 8.8/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 8.9/18.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 9.0/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 9.0/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.1/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.2/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.3/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.3/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.4/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.5/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.5/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 9.6/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 9.6/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 9.7/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 9.7/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 9.8/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 9.9/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 9.9/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 10.0/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 10.1/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 10.1/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 10.2/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 10.3/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 10.4/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 10.5/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 10.6/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 10.6/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 10.7/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 10.8/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 10.9/18.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 11.0/18.2 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 11.0/18.2 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 11.1/18.2 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 11.2/18.2 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 11.3/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.4/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.4/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.5/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.6/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.7/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.8/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.8/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 11.9/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 11.9/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 12.0/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 12.1/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 12.1/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 12.2/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 12.3/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 12.3/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 12.4/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 12.4/18.2 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 12.5/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 12.5/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 12.6/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 12.6/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 12.7/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 12.7/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 12.8/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 12.8/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 12.9/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 12.9/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 12.9/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.0/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.0/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.0/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.1/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.1/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.2/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.2/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.3/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.3/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.4/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.4/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.5/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.5/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.5/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.6/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.6/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 13.6/18.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 13.7/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 13.8/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 13.8/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 13.9/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 13.9/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 14.0/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 14.0/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 14.1/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 14.2/18.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 14.2/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.3/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.3/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.3/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.4/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.4/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.5/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.5/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.6/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.6/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.7/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.7/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.7/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.8/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.8/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.9/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.9/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 15.0/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.0/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.1/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.1/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.1/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.2/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.2/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.3/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.3/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.4/18.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 15.5/18.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 15.5/18.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 15.6/18.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 15.6/18.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 15.7/18.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 15.7/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 15.8/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 15.8/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 15.8/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 15.9/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 15.9/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 15.9/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.0/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.0/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.1/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.1/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.2/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.2/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.3/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.3/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.4/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.4/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.4/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.5/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.5/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.6/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.7/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.7/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.8/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 16.9/18.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 17.0/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.0/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.1/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.2/18.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.2/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.2/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.3/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.3/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.3/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.4/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.4/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.5/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.5/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.6/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.6/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.7/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.7/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.8/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.9/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.0/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.0/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.2/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.2/18.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rank-bm25) (1.26.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "% pip install langchain langchain-community langchain-openai\n",
    "% pip install chromadb sentence-transformers\n",
    "% pip install PyPDF2 tiktoken\n",
    "% pip install streamlit plotly\n",
    "% pip install openai anthropic\n",
    "% pip install faiss-cpu\n",
    "% pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9163efda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\OMEN\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Vector search and embeddings\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# PDF processing\n",
    "import PyPDF2\n",
    "\n",
    "# Evaluation and metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41311635",
   "metadata": {},
   "source": [
    "## 2. Traditional RAG vs Agentic RAG Comparison <a id=\"comparison\"></a>\n",
    "\n",
    "Let's understand the key differences between traditional RAG and Agentic RAG systems:\n",
    "\n",
    "### Traditional RAG:\n",
    "- **Static Pipeline**: Query → Retrieve → Generate  \n",
    "- **Single Retrieval**: One-shot retrieval based on query  \n",
    "- **No Reasoning**: Direct mapping from query to documents  \n",
    "- **Limited Context**: Fixed context window  \n",
    "\n",
    "### Agentic RAG:\n",
    "- **Dynamic Planning**: Agents decide retrieval strategy  \n",
    "- **Multi-step Reasoning**: Can perform multiple retrievals  \n",
    "- **Self-Reflection**: Agents evaluate retrieval quality  \n",
    "- **Adaptive Context**: Dynamically adjusts based on task complexity  \n",
    "- **Tool Integration**: Can use multiple tools and strategies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed865b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Any\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up a basic logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Data class to store retrieval results with metadata\"\"\"\n",
    "    content: str\n",
    "    source: str\n",
    "    score: float\n",
    "    page_number: Optional[int] = None\n",
    "    chunk_id: Optional[str] = None\n",
    "    retrieval_method: Optional[str] = None\n",
    "    timestamp: float = time.time()\n",
    "\n",
    "@dataclass\n",
    "class AgentDecision:\n",
    "    \"\"\"Data class to store agent decision-making process\"\"\"\n",
    "    action: str\n",
    "    reasoning: str\n",
    "    confidence: float\n",
    "    next_steps: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class BaseAgent(ABC):\n",
    "    \"\"\"Abstract base class for all agents in the system\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.decision_history: List[AgentDecision] = []\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, input_data: Any) -> Any:\n",
    "        \"\"\"Execute the agent's main function\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def log_decision(self, decision: AgentDecision):\n",
    "        \"\"\"Log agent decisions for analysis\"\"\"\n",
    "        self.decision_history.append(decision)\n",
    "        logger.info(f\"{self.name}: {decision.action} - {decision.reasoning}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3694622f",
   "metadata": {},
   "source": [
    "## 3. Document Processing and Indexing <a id=\"processing\"></a>\n",
    "\n",
    "Implement sophisticated document processing with multiple indexing strategies for optimal retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8359dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def load_pdf(self, file_path: str) -> List[Document]:\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add metadata\n",
    "            for i, doc in enumerate(documents):\n",
    "                doc.metadata.update({\n",
    "                    'source_file': file_path,\n",
    "                    'page_number': i + 1,\n",
    "                    'total_pages': len(documents)\n",
    "                })\n",
    "            \n",
    "            logger.info(f\"Loaded {len(documents)} pages from {file_path}\")\n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF {file_path}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def create_chunks(self, documents: List[Document]) -> List[Document]:\n",
    "        chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_chunks = self.text_splitter.split_documents([doc])\n",
    "\n",
    "            for i, chunk in enumerate(doc_chunks):\n",
    "                chunk.metadata.update({\n",
    "                    'chunk_id': f\"{doc.metadata.get('page_number', 0)}-{i}\",\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks_in_page': len(doc_chunks),\n",
    "                    'word_count': len(chunk.page_content.split()),\n",
    "                    'char_count': len(chunk.page_content)\n",
    "                })\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        logger.info(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "        return chunks\n",
    "    \n",
    "    def extract_key_phrases(self, text: str) -> List[str]:\n",
    "        import re\n",
    "      \n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "        words = re.findall(r'\\b[A-Za-z]{3,}\\b', text.lower())\n",
    "        keywords = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        phrases = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text)\n",
    "        \n",
    "        return list(set(keywords[:10] + phrases[:5]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706d2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 11 pages from data/NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
      "INFO:__main__:Created 51 chunks from 11 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 pages into 51 chunks\n",
      "Average chunk size: 690 characters\n",
      "\n",
      "Sample chunk:\n",
      "Content preview: constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling ...\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'data/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'data/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page_number': 2, 'chunk_id': '2-1', 'chunk_index': 1, 'total_chunks_in_page': 7, 'word_count': 120, 'char_count': 787}\n"
     ]
    }
   ],
   "source": [
    "# Load and process the attention paper\n",
    "processor = DocumentProcessor(chunk_size=800, chunk_overlap=100)\n",
    "\n",
    "# Load the PDF\n",
    "pdf_path = \"data/NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
    "documents = processor.load_pdf(pdf_path)\n",
    "\n",
    "# Create chunks\n",
    "chunks = processor.create_chunks(documents)\n",
    "\n",
    "print(f\"Processed {len(documents)} pages into {len(chunks)} chunks\")\n",
    "print(f\"Average chunk size: {np.mean([len(chunk.page_content) for chunk in chunks]):.0f} characters\")\n",
    "\n",
    "# Display sample chunk\n",
    "if chunks:\n",
    "    sample_chunk = chunks[5]  \n",
    "    print(\"\\nSample chunk:\")\n",
    "    print(f\"Content preview: {sample_chunk.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {sample_chunk.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c90e56",
   "metadata": {},
   "source": [
    "## 4. Multi-Modal Indexing System <a id=\"optimization\"></a>\n",
    "\n",
    "Implement multiple retrieval methods that our agents can choose from based on the query type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    def __init__(self, chunks: List[Document]):\n",
    "        self.chunks = chunks\n",
    "        self.chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self._build_semantic_index()\n",
    "        self._build_lexical_index()\n",
    "        self._build_contextual_index()\n",
    "        \n",
    "        logger.info(\"Hybrid retriever initialized with all indexes\")\n",
    "    \n",
    "    def _build_semantic_index(self):\n",
    "        print(\"Building semantic index...\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        self.embeddings = self.embeddings_model.encode(self.chunk_texts)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexFlatIP(dimension)  \n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        normalized_embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "        self.faiss_index.add(normalized_embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"Semantic index built with {len(self.chunk_texts)} chunks\")\n",
    "    \n",
    "    def _build_lexical_index(self):\n",
    "        print(\"Building lexical index...\")\n",
    "        \n",
    "        tokenized_chunks = [chunk.lower().split() for chunk in self.chunk_texts]\n",
    "        self.bm25_index = BM25Okapi(tokenized_chunks)\n",
    "        \n",
    "        print(\"Lexical index built\")\n",
    "    \n",
    "    def _build_contextual_index(self):\n",
    "        print(\"Building contextual index...\")\n",
    "        \n",
    "        self.contextual_map = {}\n",
    "        \n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            chunk_id = chunk.metadata.get('chunk_id', str(i))\n",
    "            page_num = chunk.metadata.get('page_number', 1)\n",
    "            \n",
    "            related_chunks = []\n",
    "            for j, other_chunk in enumerate(self.chunks):\n",
    "                if i != j:\n",
    "                    other_page = other_chunk.metadata.get('page_number', 1)\n",
    "                    if abs(page_num - other_page) <= 1:  \n",
    "                        related_chunks.append(j)\n",
    "            \n",
    "            self.contextual_map[i] = related_chunks\n",
    "        \n",
    "        print(\"Contextual index built\")\n",
    "    \n",
    "    def semantic_search(self, query: str, k: int = 5) -> List[RetrievalResult]:\n",
    "        query_embedding = self.embeddings_model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        scores, indices = self.faiss_index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.chunks):\n",
    "                chunk = self.chunks[idx]\n",
    "                result = RetrievalResult(\n",
    "                    content=chunk.page_content,\n",
    "                    source=chunk.metadata.get('source_file', 'unknown'),\n",
    "                    score=float(score),\n",
    "                    page_number=chunk.metadata.get('page_number'),\n",
    "                    chunk_id=chunk.metadata.get('chunk_id'),\n",
    "                    retrieval_method='semantic'\n",
    "                )\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def lexical_search(self, query: str, k: int = 5) -> List[RetrievalResult]:\n",
    "        query_tokens = query.lower().split()\n",
    "        scores = self.bm25_index.get_scores(query_tokens)\n",
    "        top_indices = np.argsort(scores)[::-1][:k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if scores[idx] > 0: \n",
    "                chunk = self.chunks[idx]\n",
    "                result = RetrievalResult(\n",
    "                    content=chunk.page_content,\n",
    "                    source=chunk.metadata.get('source_file', 'unknown'),\n",
    "                    score=float(scores[idx]),\n",
    "                    page_number=chunk.metadata.get('page_number'),\n",
    "                    chunk_id=chunk.metadata.get('chunk_id'),\n",
    "                    retrieval_method='lexical'\n",
    "                )\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def contextual_search(self, base_results: List[RetrievalResult], expand_count: int = 2) -> List[RetrievalResult]:\n",
    "        expanded_results = list(base_results)\n",
    "        \n",
    "        for result in base_results:\n",
    "            chunk_idx = None\n",
    "            for i, chunk in enumerate(self.chunks):\n",
    "                if chunk.metadata.get('chunk_id') == result.chunk_id:\n",
    "                    chunk_idx = i\n",
    "                    break\n",
    "            \n",
    "            if chunk_idx is not None and chunk_idx in self.contextual_map:\n",
    "                related_indices = self.contextual_map[chunk_idx][:expand_count]\n",
    "                \n",
    "                for rel_idx in related_indices:\n",
    "                    rel_chunk = self.chunks[rel_idx]\n",
    "                    contextual_result = RetrievalResult(\n",
    "                        content=rel_chunk.page_content,\n",
    "                        source=rel_chunk.metadata.get('source_file', 'unknown'),\n",
    "                        score=result.score * 0.8,  \n",
    "                        page_number=rel_chunk.metadata.get('page_number'),\n",
    "                        chunk_id=rel_chunk.metadata.get('chunk_id'),\n",
    "                        retrieval_method='contextual'\n",
    "                    )\n",
    "                    expanded_results.append(contextual_result)\n",
    "        \n",
    "        return expanded_results\n",
    "    \n",
    "    def hybrid_search(self, query: str, k: int = 5, weights: Dict[str, float] = None) -> List[RetrievalResult]:\n",
    "        if weights is None:\n",
    "            weights = {'semantic': 0.6, 'lexical': 0.4}\n",
    "        \n",
    "        semantic_results = self.semantic_search(query, k)\n",
    "        lexical_results = self.lexical_search(query, k)\n",
    "        combined_results = {}\n",
    "\n",
    "        for result in semantic_results:\n",
    "            key = result.chunk_id\n",
    "            if key not in combined_results:\n",
    "                combined_results[key] = result\n",
    "                combined_results[key].score = result.score * weights.get('semantic', 0.6)\n",
    "            else:\n",
    "                combined_results[key].score += result.score * weights.get('semantic', 0.6)\n",
    " \n",
    "        for result in lexical_results:\n",
    "            key = result.chunk_id\n",
    "            if key not in combined_results:\n",
    "                combined_results[key] = result\n",
    "                combined_results[key].score = result.score * weights.get('lexical', 0.4)\n",
    "            else:\n",
    "                combined_results[key].score += result.score * weights.get('lexical', 0.4)\n",
    "       \n",
    "        sorted_results = sorted(combined_results.values(), key=lambda x: x.score, reverse=True)[:k]\n",
    "\n",
    "        for result in sorted_results:\n",
    "            result.retrieval_method = 'hybrid'\n",
    "        \n",
    "        return sorted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ebf94f27774bf882c0dd595fcf3d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OMEN\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\OMEN\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2665d7643cc74b05aee7028885f0404b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ac77925a3b4673967ab212a0a4bb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c8a392c0224fd2aa3bd97af3f64bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c9423c23c443a8a7b33fb8fe5c4eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb707d3e2664df3bb506af7ac3d1e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9716896c554d55889bf93c29c6ea7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d99d28e2094b27828f274c27b7bdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5341e50fbc1e410abd0ca3c7059d14a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3259ba1156b49cfbe32176215a40cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb193ae5888a480e9b93bd14d6c985e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building semantic index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb48aeaab0784792ab9fda2ea6839283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Hybrid retriever initialized with all indexes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic index built with 51 chunks\n",
      "Building lexical index...\n",
      "Lexical index built\n",
      "Building contextual index...\n",
      "Contextual index built\n",
      "Testing different retrieval methods:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42c3e2b5a5640e89c24072ef6d315f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Search Results (3 results):\n",
      "1. Score: 0.5548 | Page: 2\n",
      "   Content: it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "r...\n",
      "\n",
      "2. Score: 0.5530 | Page: 9\n",
      "   Content: We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "...\n",
      "\n",
      "3. Score: 0.4634 | Page: 5\n",
      "   Content: MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\n",
      "where headi = Attention(QWQ\n",
      "i ,KW K\n",
      "i ,VW V\n",
      "i )\n",
      "Where...\n",
      "\n",
      "\n",
      "Lexical Search Results (3 results):\n",
      "1. Score: 6.1750 | Page: 2\n",
      "   Content: it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "r...\n",
      "\n",
      "2. Score: 5.8413 | Page: 2\n",
      "   Content: constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral...\n",
      "\n",
      "3. Score: 5.4722 | Page: 2\n",
      "   Content: textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "End-to-en...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d5ce7c919a4f0f9ecf9066845a2f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Search Results (3 results):\n",
      "1. Score: 2.8029 | Page: 2\n",
      "   Content: it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "r...\n",
      "\n",
      "2. Score: 2.3365 | Page: 2\n",
      "   Content: constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral...\n",
      "\n",
      "3. Score: 2.1889 | Page: 2\n",
      "   Content: textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "End-to-en...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = HybridRetriever(chunks)\n",
    "\n",
    "test_query = \"What is the attention mechanism in transformers?\"\n",
    "\n",
    "print(\"Testing different retrieval methods:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "semantic_results = retriever.semantic_search(test_query, k=3)\n",
    "print(f\"\\nSemantic Search Results ({len(semantic_results)} results):\")\n",
    "for i, result in enumerate(semantic_results):\n",
    "    print(f\"{i+1}. Score: {result.score:.4f} | Page: {result.page_number}\")\n",
    "    print(f\"   Content: {result.content[:100]}...\\n\")\n",
    "\n",
    "lexical_results = retriever.lexical_search(test_query, k=3)\n",
    "print(f\"\\nLexical Search Results ({len(lexical_results)} results):\")\n",
    "for i, result in enumerate(lexical_results):\n",
    "    print(f\"{i+1}. Score: {result.score:.4f} | Page: {result.page_number}\")\n",
    "    print(f\"   Content: {result.content[:100]}...\\n\")\n",
    "\n",
    "hybrid_results = retriever.hybrid_search(test_query, k=3)\n",
    "print(f\"\\nHybrid Search Results ({len(hybrid_results)} results):\")\n",
    "for i, result in enumerate(hybrid_results):\n",
    "    print(f\"{i+1}. Score: {result.score:.4f} | Page: {result.page_number}\")\n",
    "    print(f\"   Content: {result.content[:100]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e9f63",
   "metadata": {},
   "source": [
    "## 5. Agent Implementation <a id=\"agents\"></a>\n",
    "\n",
    "Implement the core agents that make our RAG system \"agentic\". Each agent has specific responsibilities and can make intelligent decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02292e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAnalysisAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"QueryAnalyzer\", \"Analyzes queries and determines retrieval strategy\")\n",
    "        self.query_patterns = {\n",
    "            'factual': ['what is', 'define', 'explain', 'describe'],\n",
    "            'comparison': ['compare', 'difference', 'versus', 'vs'],\n",
    "            'procedural': ['how to', 'steps', 'process', 'method'],\n",
    "            'analytical': ['why', 'analyze', 'evaluate', 'assess'],\n",
    "            'specific': ['when', 'where', 'who', 'which']\n",
    "        }\n",
    "    \n",
    "    def execute(self, query: str) -> AgentDecision:\n",
    "        query_lower = query.lower()\n",
    " \n",
    "        query_type = 'general'\n",
    "        confidence = 0.5\n",
    "        \n",
    "        for q_type, patterns in self.query_patterns.items():\n",
    "            if any(pattern in query_lower for pattern in patterns):\n",
    "                query_type = q_type\n",
    "                confidence = 0.8\n",
    "                break\n",
    "\n",
    "        complexity = 'simple'\n",
    "        if len(query.split()) > 10 or any(word in query_lower for word in ['and', 'or', 'but', 'however']):\n",
    "            complexity = 'complex'\n",
    "            confidence = min(confidence + 0.1, 1.0)\n",
    "\n",
    "        if query_type == 'factual':\n",
    "            strategy = 'semantic'\n",
    "            reasoning = \"Factual queries benefit from semantic understanding\"\n",
    "        elif query_type == 'specific':\n",
    "            strategy = 'lexical'\n",
    "            reasoning = \"Specific queries need exact term matching\"\n",
    "        elif complexity == 'complex':\n",
    "            strategy = 'hybrid'\n",
    "            reasoning = \"Complex queries require multi-modal retrieval\"\n",
    "        else:\n",
    "            strategy = 'hybrid'\n",
    "            reasoning = \"Default to hybrid approach for balanced results\"\n",
    "        \n",
    "        decision = AgentDecision(\n",
    "            action=f\"recommend_{strategy}_retrieval\",\n",
    "            reasoning=reasoning,\n",
    "            confidence=confidence,\n",
    "            next_steps=[f\"Execute {strategy} search\", \"Evaluate results\", \"Refine if needed\"],\n",
    "            metadata={\n",
    "                'query_type': query_type,\n",
    "                'complexity': complexity,\n",
    "                'recommended_strategy': strategy,\n",
    "                'query_length': len(query.split())\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.log_decision(decision)\n",
    "        return decision\n",
    "\n",
    "class RetrievalAgent(BaseAgent):\n",
    "    def __init__(self, retriever: HybridRetriever):\n",
    "        super().__init__(\"RetrievalAgent\", \"Executes document retrieval with various strategies\")\n",
    "        self.retriever = retriever\n",
    "        self.retrieval_history = []\n",
    "    \n",
    "    def execute(self, query: str, strategy: str = 'hybrid', k: int = 5) -> list[RetrievalResult]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if strategy == 'semantic':\n",
    "            results = self.retriever.semantic_search(query, k)\n",
    "        elif strategy == 'lexical':\n",
    "            results = self.retriever.lexical_search(query, k)\n",
    "        elif strategy == 'hybrid':\n",
    "            results = self.retriever.hybrid_search(query, k)\n",
    "        else:\n",
    "            results = self.retriever.hybrid_search(query, k)  \n",
    "        \n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        self.retrieval_history.append({\n",
    "            'query': query,\n",
    "            'strategy': strategy,\n",
    "            'results_count': len(results),\n",
    "            'retrieval_time': retrieval_time,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        \n",
    "        decision = AgentDecision(\n",
    "            action=f\"retrieved_{len(results)}_documents\",\n",
    "            reasoning=f\"Used {strategy} strategy to find relevant documents\",\n",
    "            confidence=0.8 if results else 0.3,\n",
    "            next_steps=[\"Evaluate result quality\", \"Consider refinement\"] if results else [\"Try alternative strategy\"],\n",
    "            metadata={\n",
    "                'strategy_used': strategy,\n",
    "                'retrieval_time': retrieval_time,\n",
    "                'results_count': len(results)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.log_decision(decision)\n",
    "        return results\n",
    "\n",
    "class QualityAssessmentAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"QualityAssessor\", \"Evaluates and scores retrieval result quality\")\n",
    "        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def execute(self, query: str, results: list[RetrievalResult]) -> dict:\n",
    "        if not results:\n",
    "            decision = AgentDecision(\n",
    "                action=\"low_quality_assessment\",\n",
    "                reasoning=\"No results to evaluate\",\n",
    "                confidence=1.0,\n",
    "                next_steps=[\"Recommend alternative retrieval strategy\"],\n",
    "                metadata={'quality_score': 0.0, 'issues': ['no_results']}\n",
    "            )\n",
    "            self.log_decision(decision)\n",
    "            return {'quality_score': 0.0, 'issues': ['no_results'], 'recommendation': 'retry_with_different_strategy'}\n",
    "        \n",
    "        quality_metrics = self._calculate_quality_metrics(query, results)\n",
    "        overall_score = self._calculate_overall_score(quality_metrics)\n",
    "        issues = self._identify_issues(quality_metrics)\n",
    "        recommendation = self._generate_recommendation(overall_score, issues)\n",
    "        \n",
    "        decision = AgentDecision(\n",
    "            action=f\"quality_score_{overall_score:.2f}\",\n",
    "            reasoning=f\"Assessed {len(results)} results with score {overall_score:.2f}\",\n",
    "            confidence=0.9,\n",
    "            next_steps=[recommendation] if recommendation else [\"Proceed with current results\"],\n",
    "            metadata={\n",
    "                'quality_score': overall_score,\n",
    "                'issues': issues,\n",
    "                'metrics': quality_metrics\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.log_decision(decision)\n",
    "        \n",
    "        return {\n",
    "            'quality_score': overall_score,\n",
    "            'issues': issues,\n",
    "            'recommendation': recommendation,\n",
    "            'metrics': quality_metrics\n",
    "        }\n",
    "    \n",
    "    def _calculate_quality_metrics(self, query: str, results: list[RetrievalResult]) -> dict:\n",
    "        relevance_score = np.mean([result.score for result in results]) if results else 0.0\n",
    "        \n",
    "        if len(results) > 1:\n",
    "            result_embeddings = self.embeddings_model.encode([r.content for r in results])\n",
    "            similarity_matrix = cosine_similarity(result_embeddings)\n",
    "            mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)\n",
    "            avg_similarity = similarity_matrix[mask].mean()\n",
    "            diversity_score = 1.0 - avg_similarity  \n",
    "        else:\n",
    "            diversity_score = 1.0 if results else 0.0\n",
    "\n",
    "        if results:\n",
    "            query_embedding = self.embeddings_model.encode([query])\n",
    "            result_embeddings = self.embeddings_model.encode([r.content for r in results])\n",
    "            coverage_scores = cosine_similarity(query_embedding, result_embeddings)[0]\n",
    "            coverage_score = np.max(coverage_scores)  \n",
    "        else:\n",
    "            coverage_score = 0.0\n",
    "\n",
    "        if results:\n",
    "            lengths = [len(r.content) for r in results]\n",
    "            avg_length = np.mean(lengths)\n",
    "            if 200 <= avg_length <= 2000:\n",
    "                length_score = 1.0\n",
    "            elif avg_length < 200:\n",
    "                length_score = avg_length / 200.0\n",
    "            else:\n",
    "                length_score = max(0.5, 2000 / avg_length)\n",
    "        else:\n",
    "            length_score = 0.0\n",
    "        \n",
    "        return {\n",
    "            'relevance': relevance_score,\n",
    "            'diversity': diversity_score,\n",
    "            'coverage': coverage_score,\n",
    "            'length_consistency': length_score\n",
    "        }\n",
    "    \n",
    "    def _calculate_overall_score(self, metrics: dict) -> float:\n",
    "        weights = {\n",
    "            'relevance': 0.4,\n",
    "            'diversity': 0.2,\n",
    "            'coverage': 0.3,\n",
    "            'length_consistency': 0.1\n",
    "        }\n",
    "        \n",
    "        return sum(metrics[key] * weights[key] for key in weights.keys())\n",
    "    \n",
    "    def _identify_issues(self, metrics: dict) -> list[str]:\n",
    "        issues = []\n",
    "        \n",
    "        if metrics['relevance'] < 0.3:\n",
    "            issues.append('low_relevance')\n",
    "        if metrics['diversity'] < 0.2:\n",
    "            issues.append('low_diversity')\n",
    "        if metrics['coverage'] < 0.4:\n",
    "            issues.append('poor_coverage')\n",
    "        if metrics['length_consistency'] < 0.5:\n",
    "            issues.append('inconsistent_length')\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def _generate_recommendation(self, score: float, issues: list[str]) -> str | None:\n",
    "        if score < 0.5:\n",
    "            if 'low_relevance' in issues:\n",
    "                return 'try_different_retrieval_strategy'\n",
    "            elif 'poor_coverage' in issues:\n",
    "                return 'expand_search_or_rephrase_query'\n",
    "            else:\n",
    "                return 'refine_retrieval_parameters'\n",
    "        elif score < 0.7 and 'low_diversity' in issues:\n",
    "            return 'add_contextual_expansion'\n",
    "        \n",
    "        return None  \n",
    "\n",
    "class CitationAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"CitationAgent\", \"Generates citations and tracks sources\")\n",
    "        self.citation_style = 'academic' \n",
    "    \n",
    "    def execute(self, results: list[RetrievalResult]) -> dict:\n",
    "        citations = []\n",
    "        source_map = {}\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            citation = self._generate_citation(result, i + 1)\n",
    "            citations.append(citation)\n",
    "            \n",
    "            source_key = f\"source_{i + 1}\"\n",
    "            source_map[source_key] = {\n",
    "                'file': result.source,\n",
    "                'page': result.page_number,\n",
    "                'chunk_id': result.chunk_id,\n",
    "                'retrieval_method': result.retrieval_method,\n",
    "                'score': result.score\n",
    "            }\n",
    "        \n",
    "        decision = AgentDecision(\n",
    "            action=f\"generated_{len(citations)}_citations\",\n",
    "            reasoning=\"Created proper citations for all retrieved sources\",\n",
    "            confidence=0.95,\n",
    "            next_steps=[\"Include citations in response\"],\n",
    "            metadata={\n",
    "                'citation_count': len(citations),\n",
    "                'citation_style': self.citation_style\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.log_decision(decision)\n",
    "        \n",
    "        return {\n",
    "            'citations': citations,\n",
    "            'source_map': source_map,\n",
    "            'bibliography': self._generate_bibliography(results)\n",
    "        }\n",
    "    \n",
    "    def _generate_citation(self, result: RetrievalResult, ref_number: int) -> str:\n",
    "        filename = Path(result.source).stem if result.source else \"Unknown\"\n",
    "        \n",
    "        if self.citation_style == 'academic':\n",
    "            if result.page_number:\n",
    "                return f\"[{ref_number}] {filename}, page {result.page_number}\"\n",
    "            else:\n",
    "                return f\"[{ref_number}] {filename}\"\n",
    "        else:\n",
    "            return f\"({ref_number}) {filename}\"\n",
    "    \n",
    "    def _generate_bibliography(self, results: List[RetrievalResult]) -> List[str]:\n",
    "        unique_sources = set()\n",
    "        bibliography = []\n",
    "        \n",
    "        for result in results:\n",
    "            source_key = (result.source, result.page_number)\n",
    "            if source_key not in unique_sources:\n",
    "                unique_sources.add(source_key)\n",
    "                filename = Path(result.source).stem if result.source else \"Unknown Source\"\n",
    "                entry = f\"{filename}\"\n",
    "                if result.page_number:\n",
    "                    entry += f\", page {result.page_number}\"\n",
    "                bibliography.append(entry)\n",
    "        \n",
    "        return bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c205fd7",
   "metadata": {},
   "source": [
    "## 6. Orchestration Agent - The Central Controller <a id=\"orchestration\"></a>\n",
    "\n",
    "This agent coordinates all other agents and implements the core agentic behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OrchestrationAgent(BaseAgent):\n",
    "#     \"\"\"Central agent that coordinates all other agents and implements agentic behavior\"\"\"\n",
    "    \n",
    "#     def __init__(self, retriever: HybridRetriever):\n",
    "#         super().__init__(\"Orchestrator\", \"Central coordinator for agentic RAG system\")\n",
    "        \n",
    "#         # Initialize all sub-agents\n",
    "#         self.query_analyzer = QueryAnalysisAgent()\n",
    "#         self.retrieval_agent = RetrievalAgent(retriever)\n",
    "#         self.quality_assessor = QualityAssessmentAgent()\n",
    "#         self.citation_agent = CitationAgent()\n",
    "        \n",
    "#         # System configuration\n",
    "#         self.max_iterations = 3\n",
    "#         self.quality_threshold = 0.6\n",
    "#         self.conversation_history = []\n",
    "    \n",
    "#     def execute(self, query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "#         \"\"\"Execute the full agentic RAG pipeline\"\"\"\n",
    "#         context = context or {}\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Store conversation\n",
    "#         conversation_id = len(self.conversation_history)\n",
    "#         conversation = {\n",
    "#             'id': conversation_id,\n",
    "#             'query': query,\n",
    "#             'context': context,\n",
    "#             'start_time': start_time,\n",
    "#             'iterations': [],\n",
    "#             'final_results': None\n",
    "#         }\n",
    "        \n",
    "#         logger.info(f\"Starting agentic RAG for query: {query[:50]}...\")\n",
    "        \n",
    "#         # Phase 1: Query Analysis\n",
    "#         analysis_decision = self.query_analyzer.execute(query)\n",
    "        \n",
    "#         # Phase 2: Iterative Retrieval and Quality Assessment\n",
    "#         best_results = []\n",
    "#         best_quality_score = 0.0\n",
    "        \n",
    "#         for iteration in range(self.max_iterations):\n",
    "#             iteration_start = time.time()\n",
    "            \n",
    "#             # Determine retrieval strategy for this iteration\n",
    "#             if iteration == 0:\n",
    "#                 strategy = analysis_decision.metadata['recommended_strategy']\n",
    "#             else:\n",
    "#                 # Adapt strategy based on previous results\n",
    "#                 strategy = self._adapt_strategy(iteration, best_quality_score)\n",
    "            \n",
    "#             # Execute retrieval\n",
    "#             results = self.retrieval_agent.execute(query, strategy, k=5)\n",
    "            \n",
    "#             # Assess quality\n",
    "#             quality_assessment = self.quality_assessor.execute(query, results)\n",
    "#             current_quality_score = quality_assessment['quality_score']\n",
    "            \n",
    "#             # Store iteration results\n",
    "#             iteration_data = {\n",
    "#                 'iteration': iteration + 1,\n",
    "#                 'strategy': strategy,\n",
    "#                 'results_count': len(results),\n",
    "#                 'quality_score': current_quality_score,\n",
    "#                 'issues': quality_assessment['issues'],\n",
    "#                 'recommendation': quality_assessment['recommendation'],\n",
    "#                 'time': time.time() - iteration_start\n",
    "#             }\n",
    "#             conversation['iterations'].append(iteration_data)\n",
    "            \n",
    "#             # Update best results if current iteration is better\n",
    "#             if current_quality_score > best_quality_score:\n",
    "#                 best_results = results\n",
    "#                 best_quality_score = current_quality_score\n",
    "            \n",
    "#             logger.info(f\"Iteration {iteration + 1}: Strategy={strategy}, Quality={current_quality_score:.3f}\")\n",
    "            \n",
    "#             # Check if we should stop iterating\n",
    "#             if self._should_stop_iteration(current_quality_score, quality_assessment, iteration):\n",
    "#                 break\n",
    "        \n",
    "#         # Phase 3: Generate Citations\n",
    "#         citation_data = self.citation_agent.execute(best_results)\n",
    "        \n",
    "#         # Phase 4: Compile Final Response\n",
    "#         final_response = self._compile_response(\n",
    "#             query, best_results, best_quality_score, citation_data, conversation\n",
    "#         )\n",
    "        \n",
    "#         # Store final results\n",
    "#         conversation['final_results'] = final_response\n",
    "#         conversation['total_time'] = time.time() - start_time\n",
    "#         self.conversation_history.append(conversation)\n",
    "        \n",
    "#         # Log final decision\n",
    "#         final_decision = AgentDecision(\n",
    "#             action=\"completed_agentic_rag\",\n",
    "#             reasoning=f\"Completed RAG with {len(conversation['iterations'])} iterations, final quality: {best_quality_score:.3f}\",\n",
    "#             confidence=min(best_quality_score + 0.2, 1.0),\n",
    "#             next_steps=[\"Present results to user\"],\n",
    "#             metadata={\n",
    "#                 'total_time': conversation['total_time'],\n",
    "#                 'iterations_used': len(conversation['iterations']),\n",
    "#                 'final_quality': best_quality_score,\n",
    "#                 'conversation_id': conversation_id\n",
    "#             }\n",
    "#         )\n",
    "#         self.log_decision(final_decision)\n",
    "        \n",
    "#         return final_response\n",
    "    \n",
    "#     def _adapt_strategy(self, iteration: int, previous_quality: float) -> str:\n",
    "#         \"\"\"Adapt retrieval strategy based on previous results\"\"\"\n",
    "#         if previous_quality < 0.4:\n",
    "#             # Low quality, try different approach\n",
    "#             strategies = ['hybrid', 'semantic', 'lexical']\n",
    "#             return strategies[iteration % len(strategies)]\n",
    "#         elif previous_quality < 0.6:\n",
    "#             # Medium quality, try hybrid approach\n",
    "#             return 'hybrid'\n",
    "#         else:\n",
    "#             # Good quality, stick with what works\n",
    "#             return 'semantic'\n",
    "    \n",
    "#     def _should_stop_iteration(self, quality_score: float, assessment: Dict, iteration: int) -> bool:\n",
    "#         \"\"\"Determine if we should stop iterating\"\"\"\n",
    "#         # Stop if quality is good enough\n",
    "#         if quality_score >= self.quality_threshold:\n",
    "#             return True\n",
    "        \n",
    "#         # Stop if no recommendation for improvement\n",
    "#         if not assessment['recommendation']:\n",
    "#             return True\n",
    "        \n",
    "#         # Continue if we have iterations left and there's room for improvement\n",
    "#         return False\n",
    "    \n",
    "#     def _compile_response(self, query: str, results: List[RetrievalResult], \n",
    "#                          quality_score: float, citation_data: Dict, \n",
    "#                          conversation: Dict) -> Dict[str, Any]:\n",
    "#         \"\"\"Compile the final response with all metadata\"\"\"\n",
    "        \n",
    "#         # Extract key information from results\n",
    "#         content_summary = self._summarize_content(results)\n",
    "        \n",
    "#         return {\n",
    "#             'query': query,\n",
    "#             'results': [asdict(result) for result in results],\n",
    "#             'content_summary': content_summary,\n",
    "#             'quality_metrics': {\n",
    "#                 'overall_score': quality_score,\n",
    "#                 'confidence': min(quality_score + 0.2, 1.0),\n",
    "#                 'result_count': len(results)\n",
    "#             },\n",
    "#             'citations': citation_data['citations'],\n",
    "#             'bibliography': citation_data['bibliography'],\n",
    "#             'process_metadata': {\n",
    "#                 'iterations_used': len(conversation['iterations']),\n",
    "#                 'total_time': conversation.get('total_time', 0),\n",
    "#                 'strategies_tried': [iter_data['strategy'] for iter_data in conversation['iterations']],\n",
    "#                 'conversation_id': conversation['id']\n",
    "#             },\n",
    "#             'agent_decisions': {\n",
    "#                 'query_analysis': self.query_analyzer.decision_history[-1] if self.query_analyzer.decision_history else None,\n",
    "#                 'retrieval_decisions': self.retrieval_agent.decision_history[-3:],  # Last 3 decisions\n",
    "#                 'quality_assessments': self.quality_assessor.decision_history[-3:],\n",
    "#                 'orchestration': self.decision_history[-1] if self.decision_history else None\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     def _summarize_content(self, results: List[RetrievalResult]) -> str:\n",
    "#         \"\"\"Create a summary of the retrieved content\"\"\"\n",
    "#         if not results:\n",
    "#             return \"No relevant content found.\"\n",
    "        \n",
    "#         # Combine content from top results\n",
    "#         combined_content = \"\\n\\n\".join([result.content for result in results[:3]])\n",
    "        \n",
    "#         # Simple extractive summary (can be enhanced with proper summarization models)\n",
    "#         sentences = combined_content.split('. ')\n",
    "#         # Return first few sentences as summary\n",
    "#         summary = '. '.join(sentences[:3]) + '.' if len(sentences) >= 3 else combined_content\n",
    "        \n",
    "#         return summary[:500] + \"...\" if len(summary) > 500 else summary\n",
    "    \n",
    "#     def get_conversation_history(self) -> List[Dict]:\n",
    "#         \"\"\"Return conversation history for analysis\"\"\"\n",
    "#         return self.conversation_history\n",
    "    \n",
    "#     def get_system_stats(self) -> Dict[str, Any]:\n",
    "#         \"\"\"Get system performance statistics\"\"\"\n",
    "#         if not self.conversation_history:\n",
    "#             return {'status': 'No conversations yet'}\n",
    "        \n",
    "#         total_conversations = len(self.conversation_history)\n",
    "#         avg_iterations = np.mean([len(conv['iterations']) for conv in self.conversation_history])\n",
    "#         avg_quality = np.mean([conv['final_results']['quality_metrics']['overall_score'] \n",
    "#                               for conv in self.conversation_history if conv['final_results']])\n",
    "#         avg_time = np.mean([conv.get('total_time', 0) for conv in self.conversation_history])\n",
    "        \n",
    "#         return {\n",
    "#             'total_conversations': total_conversations,\n",
    "#             'average_iterations': avg_iterations,\n",
    "#             'average_quality_score': avg_quality,\n",
    "#             'average_response_time': avg_time,\n",
    "#             'retrieval_agent_calls': len(self.retrieval_agent.retrieval_history),\n",
    "#             'quality_assessments': len(self.quality_assessor.decision_history)\n",
    "#         }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Installing required packages for HuggingFace integration...\n"
     ]
    }
   ],
   "source": [
    "# 1. Free Hugging Face LLM Integration\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "class HuggingFaceLLMGenerator(BaseAgent):\n",
    "    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
    "        super().__init__(\"HF_LLMGenerator\", \"Generates responses using Hugging Face models\")\n",
    "        \n",
    "        self.available_models = {\n",
    "            \"microsoft/DialoGPT-medium\": \"Conversational model\",\n",
    "            \"google/flan-t5-base\": \"Text-to-text model (recommended)\",\n",
    "            \"facebook/opt-350m\": \"Lightweight causal LM\",\n",
    "            \"distilgpt2\": \"Lightweight GPT-2 variant\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"Loading model: {model_name}\")\n",
    "            \n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "            self.generator = pipeline(\n",
    "                \"text-generation\" if \"gpt\" in model_name.lower() or \"opt\" in model_name.lower() else \"text2text-generation\",\n",
    "                model=model_name,\n",
    "                device=device,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            )\n",
    "            \n",
    "            self.model_name = model_name\n",
    "            self.model_loaded = True\n",
    "            print(f\"✅ Model {model_name} loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {model_name}: {e}\")\n",
    "            print(\"🔄 Falling back to rule-based generation...\")\n",
    "            self.model_loaded = False\n",
    "    \n",
    "    def execute(self, query: str, retrieved_content: List[RetrievalResult], \n",
    "                citations: List[str]) -> Dict[str, Any]:\n",
    "       \n",
    "        if self.model_loaded:\n",
    "            response = self._generate_hf_response(query, retrieved_content, citations)\n",
    "        else:\n",
    "            response = self._generate_rule_based_response(query, retrieved_content, citations)\n",
    "        \n",
    "        decision = AgentDecision(\n",
    "            action=\"generated_response\",\n",
    "            reasoning=f\"Generated response using {'HF model' if self.model_loaded else 'rule-based'} approach\",\n",
    "            confidence=0.85 if self.model_loaded else 0.7,\n",
    "            next_steps=[\"Present final response\"],\n",
    "            metadata={\n",
    "                'response_length': len(response['content']),\n",
    "                'sources_used': len(retrieved_content),\n",
    "                'generation_method': response['method']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.log_decision(decision)\n",
    "        return response\n",
    "    \n",
    "    def _generate_hf_response(self, query: str, retrieved_content: List[RetrievalResult], \n",
    "                             citations: List[str]) -> Dict[str, Any]:\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(retrieved_content[:3]):  \n",
    "            context_parts.append(f\"Source {i+1}: {result.content[:200]}...\")\n",
    "        \n",
    "        context = \" \".join(context_parts)\n",
    "        \n",
    "        if \"flan\" in self.model_name.lower():\n",
    "            prompt = f\"Answer the question based on the context.\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "        else:\n",
    "            prompt = f\"Based on the research context: {context[:400]}...\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "        \n",
    "        try:\n",
    "            if \"flan\" in self.model_name.lower():\n",
    "                outputs = self.generator(prompt, max_length=200, num_return_sequences=1)\n",
    "                content = outputs[0]['generated_text'].strip()\n",
    "            else:\n",
    "                outputs = self.generator(\n",
    "                    prompt, \n",
    "                    max_new_tokens=150,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.generator.tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_text = outputs[0]['generated_text']\n",
    "                content = generated_text[len(prompt):].strip()\n",
    "            \n",
    "            return {\n",
    "                'content': content,\n",
    "                'method': 'hf_generated',\n",
    "                'model_used': self.model_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"HF generation failed: {e}\")\n",
    "            return self._generate_rule_based_response(query, retrieved_content, citations)\n",
    "    \n",
    "    def _generate_rule_based_response(self, query: str, retrieved_content: List[RetrievalResult], \n",
    "                                     citations: List[str]) -> Dict[str, Any]:\n",
    " \n",
    "        if not retrieved_content:\n",
    "            return {\n",
    "                'content': \"I couldn't find relevant information to answer your question.\",\n",
    "                'method': 'rule_based',\n",
    "                'confidence': 0.1\n",
    "            }\n",
    "        \n",
    "        key_sentences = []\n",
    "        important_terms = []\n",
    "        \n",
    "        query_terms = set(word.lower() for word in query.split() if len(word) > 3)\n",
    "        \n",
    "        for result in retrieved_content[:3]:\n",
    "            sentences = [s.strip() for s in result.content.split('.') if len(s.strip()) > 20]\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence_terms = set(word.lower() for word in sentence.split() if len(word) > 3)\n",
    "                overlap = len(query_terms.intersection(sentence_terms))\n",
    "                \n",
    "                if overlap >= 2:  \n",
    "                    key_sentences.append(sentence)\n",
    "                    caps_words = [w for w in sentence.split() if w[0].isupper() and len(w) > 3]\n",
    "                    important_terms.extend(caps_words[:3])\n",
    "        \n",
    "        if key_sentences:\n",
    "            response_parts = [\n",
    "                f\"Based on the retrieved research documents about {query.lower()}:\",\n",
    "                \"\",\n",
    "                \". \".join(key_sentences[:2]) + \".\",\n",
    "                \"\",\n",
    "                \"Key concepts mentioned:\",\n",
    "                \", \".join(list(set(important_terms))[:5]) if important_terms else \"Various technical concepts\",\n",
    "                \"\",\n",
    "                f\"Additional context from page {retrieved_content[0].page_number}:\",\n",
    "                retrieved_content[0].content[:150] + \"...\"\n",
    "            ]\n",
    "            \n",
    "            content = \"\\n\".join(response_parts)\n",
    "        else:\n",
    "            content = (\n",
    "                f\"Based on the available research documentation:\\n\\n\"\n",
    "                f\"{retrieved_content[0].content[:200]}...\\n\\n\"\n",
    "                f\"This information comes from page {retrieved_content[0].page_number} \"\n",
    "                f\"of the research paper.\"\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'content': content,\n",
    "            'method': 'enhanced_rule_based',\n",
    "            'sentences_used': len(key_sentences)\n",
    "        }\n",
    "\n",
    "class QualityAssessmentAgent(QualityAssessmentAgent):\n",
    "    def _calculate_quality_metrics(self, query: str, results: list[RetrievalResult]) -> dict:\n",
    "        if not results:\n",
    "            return {\n",
    "                'relevance': 0.0,\n",
    "                'diversity': 0.0,\n",
    "                'coverage': 0.0,\n",
    "                'length_consistency': 0.0,\n",
    "                'term_overlap': 0.0\n",
    "            }\n",
    "        \n",
    "        query_terms = set(word.lower() for word in query.split() if len(word) > 2)\n",
    "    \n",
    "        relevance_scores = []\n",
    "        for result in results:\n",
    "            normalized_score = min(result.score, 1.0) if result.score > 0 else 0.0\n",
    "            relevance_scores.append(normalized_score)\n",
    "        \n",
    "        relevance_score = np.mean(relevance_scores) if relevance_scores else 0.0\n",
    "        \n",
    "        term_overlap_scores = []\n",
    "        for result in results:\n",
    "            content_terms = set(word.lower() for word in result.content.split() if len(word) > 2)\n",
    "            if query_terms:\n",
    "                overlap_ratio = len(query_terms.intersection(content_terms)) / len(query_terms)\n",
    "                term_overlap_scores.append(overlap_ratio)\n",
    "        \n",
    "        term_overlap_score = np.mean(term_overlap_scores) if term_overlap_scores else 0.0\n",
    "        \n",
    "        if len(results) > 1:\n",
    "            # Check for different pages\n",
    "            unique_pages = len(set(r.page_number for r in results if r.page_number))\n",
    "            page_diversity = min(unique_pages / len(results), 1.0)\n",
    "            \n",
    "            # Check content diversity \n",
    "            content_lengths = [len(r.content) for r in results]\n",
    "            length_variance = np.var(content_lengths) / np.mean(content_lengths) if content_lengths else 0\n",
    "            length_diversity = min(length_variance, 1.0)\n",
    "            \n",
    "            diversity_score = (page_diversity + length_diversity) / 2\n",
    "        else:\n",
    "            diversity_score = 0.5  \n",
    "        \n",
    "        # 4. Coverage score \n",
    "        coverage_score = min((relevance_score + term_overlap_score) / 2, 1.0)\n",
    "        \n",
    "        # 5. Length consistency\n",
    "        if results:\n",
    "            lengths = [len(r.content) for r in results]\n",
    "            avg_length = np.mean(lengths)\n",
    "            # Prefer lengths between 100-1000 characters\n",
    "            if 100 <= avg_length <= 1000:\n",
    "                length_score = 1.0\n",
    "            elif avg_length < 100:\n",
    "                length_score = avg_length / 100.0\n",
    "            else:\n",
    "                length_score = max(0.3, 1000 / avg_length)\n",
    "        else:\n",
    "            length_score = 0.0\n",
    "        \n",
    "        return {\n",
    "            'relevance': relevance_score,\n",
    "            'diversity': diversity_score,\n",
    "            'coverage': coverage_score,\n",
    "            'length_consistency': length_score,\n",
    "            'term_overlap': term_overlap_score\n",
    "        }\n",
    "    \n",
    "    def _calculate_overall_score(self, metrics: dict) -> float:\n",
    "        weights = {\n",
    "            'relevance': 0.3,\n",
    "            'term_overlap': 0.25, \n",
    "            'coverage': 0.2,\n",
    "            'diversity': 0.15,\n",
    "            'length_consistency': 0.1\n",
    "        }\n",
    "        \n",
    "        score = sum(metrics.get(key, 0) * weights[key] for key in weights.keys())\n",
    "        return min(score, 1.0) \n",
    "    \n",
    "    def _identify_issues(self, metrics: dict) -> list[str]:\n",
    "        issues = []\n",
    "        \n",
    "        if metrics['relevance'] < 0.4:\n",
    "            issues.append('low_relevance')\n",
    "        if metrics['term_overlap'] < 0.3:\n",
    "            issues.append('poor_term_overlap')\n",
    "        if metrics['diversity'] < 0.3:\n",
    "            issues.append('low_diversity')\n",
    "        if metrics['coverage'] < 0.4:\n",
    "            issues.append('poor_coverage')\n",
    "        if metrics['length_consistency'] < 0.5:\n",
    "            issues.append('inconsistent_length')\n",
    "        \n",
    "        return issues\n",
    "\n",
    "# 3. Orchestration Agent \n",
    "class OrchestrationAgent(BaseAgent):\n",
    "    def __init__(self, retriever: HybridRetriever, use_hf_llm: bool = True):\n",
    "        super().__init__(retriever)\n",
    "        \n",
    "        self.quality_assessor = QualityAssessmentAgent()\n",
    "        \n",
    "        if use_hf_llm:\n",
    "            models_to_try = [\n",
    "                \"google/flan-t5-base\", \n",
    "                \"distilgpt2\",           \n",
    "                \"microsoft/DialoGPT-medium\"  \n",
    "            ]\n",
    "            \n",
    "            self.llm_generator = None\n",
    "            for model in models_to_try:\n",
    "                try:\n",
    "                    self.llm_generator = HuggingFaceLLMGenerator(model)\n",
    "                    if self.llm_generator.model_loaded:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not self.llm_generator or not self.llm_generator.model_loaded:\n",
    "                print(\"⚠️ No HF model could be loaded, using enhanced rule-based generation\")\n",
    "                self.llm_generator = HuggingFaceLLMGenerator(\"fallback\")  \n",
    "        else:\n",
    "            self.llm_generator = HuggingFaceLLMGenerator(\"fallback\")\n",
    "        \n",
    "        # Fixed configuration\n",
    "        self.max_iterations = 3\n",
    "        self.quality_threshold = 0.5  \n",
    "        self.min_iterations = 2\n",
    "    \n",
    "    def _should_stop_iteration(self, quality_score: float, assessment: Dict, iteration: int) -> bool:\n",
    "        if iteration < self.min_iterations - 1:\n",
    "            return False\n",
    "        \n",
    "        if quality_score >= 0.75:\n",
    "            return True\n",
    "        \n",
    "        if iteration >= self.max_iterations - 1:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _adapt_strategy(self, iteration: int, previous_quality: float) -> str:\n",
    "        if iteration == 1:\n",
    "            return 'hybrid'\n",
    "        elif iteration == 2:\n",
    "            if previous_quality < 0.5:\n",
    "                return 'lexical' \n",
    "            else:\n",
    "                return 'semantic'  \n",
    "        else:\n",
    "            return 'hybrid'  \n",
    "    \n",
    "    def execute(self, query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        context = context or {}\n",
    "        start_time = time.time()\n",
    "        \n",
    "        conversation_id = len(self.conversation_history)\n",
    "        conversation = {\n",
    "            'id': conversation_id,\n",
    "            'query': query,\n",
    "            'context': context,\n",
    "            'start_time': start_time,\n",
    "            'iterations': [],\n",
    "            'final_results': None\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"🚀 Starting FIXED agentic RAG for query: {query[:50]}...\")\n",
    " \n",
    "        analysis_decision = self.query_analyzer.execute(query)\n",
    "        best_results = []\n",
    "        best_quality_score = 0.0\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            iteration_start = time.time()\n",
    "            \n",
    "            if iteration == 0:\n",
    "                strategy = analysis_decision.metadata['recommended_strategy']\n",
    "            else:\n",
    "                strategy = self._adapt_strategy(iteration, best_quality_score)\n",
    "\n",
    "            k = 5 + iteration  \n",
    "            results = self.retrieval_agent.execute(query, strategy, k=k)\n",
    "            quality_assessment = self.quality_assessor.execute(query, results)\n",
    "            current_quality_score = quality_assessment['quality_score']\n",
    "            \n",
    "            iteration_data = {\n",
    "                'iteration': iteration + 1,\n",
    "                'strategy': strategy,\n",
    "                'results_count': len(results),\n",
    "                'quality_score': current_quality_score,\n",
    "                'issues': quality_assessment['issues'],\n",
    "                'recommendation': quality_assessment['recommendation'],\n",
    "                'time': time.time() - iteration_start\n",
    "            }\n",
    "            conversation['iterations'].append(iteration_data)\n",
    "\n",
    "            if current_quality_score > best_quality_score:\n",
    "                best_results = results\n",
    "                best_quality_score = current_quality_score\n",
    "            \n",
    "            logger.info(f\"🔄 Iteration {iteration + 1}: Strategy={strategy}, Quality={current_quality_score:.3f}\")\n",
    "      \n",
    "            if self._should_stop_iteration(current_quality_score, quality_assessment, iteration):\n",
    "                logger.info(f\"🛑 Stopping after {iteration + 1} iterations\")\n",
    " \n",
    "        citation_data = self.citation_agent.execute(best_results)\n",
    "\n",
    "        if self.llm_generator:\n",
    "            llm_response = self.llm_generator.execute(query, best_results, citation_data['citations'])\n",
    "        else:\n",
    "            llm_response = {'content': self._summarize_content(best_results), 'method': 'summary'}\n",
    "\n",
    "        final_response = self._compile_enhanced_response(\n",
    "            query, best_results, best_quality_score, citation_data, llm_response, conversation\n",
    "        )\n",
    "\n",
    "        conversation['final_results'] = final_response\n",
    "        conversation['total_time'] = time.time() - start_time\n",
    "        self.conversation_history.append(conversation)\n",
    " \n",
    "        final_decision = AgentDecision(\n",
    "            action=\"completed_fixed_agentic_rag\",\n",
    "            reasoning=f\"✅ Completed FIXED RAG with {len(conversation['iterations'])} iterations, final quality: {best_quality_score:.3f}\",\n",
    "            confidence=min(best_quality_score + 0.2, 1.0),\n",
    "            next_steps=[\"Present results to user\"],\n",
    "            metadata={\n",
    "                'total_time': conversation['total_time'],\n",
    "                'iterations_used': len(conversation['iterations']),\n",
    "                'final_quality': best_quality_score,\n",
    "                'conversation_id': conversation_id\n",
    "            }\n",
    "        )\n",
    "        self.log_decision(final_decision)\n",
    "        \n",
    "        return final_response\n",
    "    \n",
    "    def _compile_enhanced_response(self, query: str, results: List[RetrievalResult], \n",
    "                                  quality_score: float, citation_data: Dict, \n",
    "                                  llm_response: Dict, conversation: Dict) -> Dict[str, Any]:\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'response': llm_response['content'],\n",
    "            'generation_method': llm_response['method'],\n",
    "            'results': [asdict(result) for result in results],\n",
    "            'content_summary': self._summarize_content(results),\n",
    "            'quality_metrics': {\n",
    "                'overall_score': quality_score,\n",
    "                'confidence': min(quality_score + 0.2, 1.0),\n",
    "                'result_count': len(results)\n",
    "            },\n",
    "            'citations': citation_data['citations'],\n",
    "            'bibliography': citation_data['bibliography'],\n",
    "            'process_metadata': {\n",
    "                'iterations_used': len(conversation['iterations']),\n",
    "                'total_time': conversation.get('total_time', 0),\n",
    "                'strategies_tried': [iter_data['strategy'] for iter_data in conversation['iterations']],\n",
    "                'conversation_id': conversation['id']\n",
    "            },\n",
    "            'agent_decisions': {\n",
    "                'query_analysis': self.query_analyzer.decision_history[-1] if self.query_analyzer.decision_history else None,\n",
    "                'retrieval_decisions': self.retrieval_agent.decision_history[-3:],\n",
    "                'quality_assessments': self.quality_assessor.decision_history[-3:],\n",
    "                'llm_generation': self.llm_generator.decision_history[-1] if self.llm_generator and self.llm_generator.decision_history else None,\n",
    "                'orchestration': self.decision_history[-1] if self.decision_history else None\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"🔧 Installing required packages for HuggingFace integration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a68365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing FIXED Agentic RAG System with HuggingFace LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/flan-t5-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19581e5d4714de6902427cc1d2d8c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OMEN\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\OMEN\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579ca84c8ef45d48525ed0d1b1c2f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9828a7bafb45b4ae0ac94db68c4427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598ac64a6bf04e57b448ac64c974b66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6aac5f160544314ad1cd26f3a7f8678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aa5fada53b466185e918bb0b742120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3e2eabad004bb99f880080b395c0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: What is the attention mechanism in transformers?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model google/flan-t5-base loaded successfully!\n",
      "\n",
      "============================================================\n",
      "TESTING FIXED AGENTIC RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "🔍 Test 1: What is the attention mechanism in transformers?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b41e4c190874f8d908d70fa46c1f2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.59 - Assessed 5 results with score 0.59\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=semantic, Quality=0.587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa32d97ab5e948a38c7e39bd88d666ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.79 - Assessed 6 results with score 0.79\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.787\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_6_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.787\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: How does multi-head attention work?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Complex queries require multi-modal retrieval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quality Score: 0.787\n",
      "🔄 Iterations Used: 2\n",
      "🤖 Generation Method: hf_generated\n",
      "⏱️ Total Time: 0.00s\n",
      "\n",
      "📝 Generated Response:\n",
      "a recurrent attention mechanism instead of sequence- aligned recurren\n",
      "\n",
      "📚 Citations:\n",
      "  • [1] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "  • [2] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "  • [3] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔍 Test 2: How does multi-head attention work?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8d7157f2bf4f5fb0cec04b36991d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.72 - Assessed 5 results with score 0.72\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12095b43d684e84acc10cce0bb3135e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.71 - Assessed 6 results with score 0.71\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b676d1e59a604dd280a20e168b672acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_7_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.53 - Assessed 7 results with score 0.53\n",
      "INFO:__main__:🔄 Iteration 3: Strategy=semantic, Quality=0.526\n",
      "INFO:__main__:🛑 Stopping after 3 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 3 iterations, final quality: 0.724\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: Compare scaled dot-product attention with additive...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quality Score: 0.724\n",
      "🔄 Iterations Used: 3\n",
      "🤖 Generation Method: hf_generated\n",
      "⏱️ Total Time: 0.00s\n",
      "\n",
      "📝 Generated Response:\n",
      "Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "📚 Citations:\n",
      "  • [1] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "  • [2] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "  • [3] NIPS-2017-attention-is-all-you-need-Paper, page 7\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔍 Test 3: Compare scaled dot-product attention with additive attention\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae164664e3b464a8775d3e493f4b21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.86 - Assessed 5 results with score 0.86\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab4503322374ba6a782b5b5a4b098bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.86 - Assessed 6 results with score 0.86\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.858\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quality Score: 0.865\n",
      "🔄 Iterations Used: 2\n",
      "🤖 Generation Method: hf_generated\n",
      "⏱️ Total Time: 0.00s\n",
      "\n",
      "📝 Generated Response:\n",
      "The relevant information to answer the above question is: Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Proproduct Attention\" (Figure 2). The input consists of queries and keys of dimension dk,... Source 3: of 1dk . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical com...\n",
      "\n",
      "📚 Citations:\n",
      "  • [1] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "  • [2] NIPS-2017-attention-is-all-you-need-Paper, page 3\n",
      "  • [3] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "\n",
      "📊 PERFORMANCE COMPARISON:\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638c891dea4e43bfa3381afccf71456e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QualityAssessor: quality_score_0.59 - Assessed 5 results with score 0.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f287f60c144431485a406c35c67feb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QualityAssessor: quality_score_0.56 - Assessed 5 results with score 0.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7e97a49c2742e2acdcb1eb5c9d6c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QualityAssessor: quality_score_0.70 - Assessed 5 results with score 0.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Agentic RAG Quality: 0.865\n",
      "Fixed Agentic RAG Avg Iterations: 2.0\n",
      "Fixed Agentic RAG Avg Time: 16.772s\n",
      "\n",
      "📈 System Statistics:\n",
      "  Total Conversations: 3\n",
      "  Average Iterations: 2.333\n",
      "  Average Quality Score: 0.792\n",
      "  Average Response Time: 9.775\n",
      "  Retrieval Agent Calls: 7\n",
      "  Quality Assessments: 10\n",
      "\n",
      "🎯 Expected Improvements:\n",
      "  ✅ Minimum 2 iterations guaranteed\n",
      "  ✅ Better quality scoring calibration\n",
      "  ✅ HuggingFace LLM integration (free)\n",
      "  ✅ Enhanced response generation\n",
      "  ✅ Improved strategy adaptation\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Initializing Agentic RAG System with HuggingFace LLM...\")\n",
    "\n",
    "# Initialize the orchestrator\n",
    "orchestrator = OrchestrationAgent(retriever, use_hf_llm=True)\n",
    "\n",
    "# Test with the same queries\n",
    "test_queries = [\n",
    "    \"What is the attention mechanism in transformers?\",\n",
    "    \"How does multi-head attention work?\",\n",
    "    \"Compare scaled dot-product attention with additive attention\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING AGENTIC RAG SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n🔍 Test {idx}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    response = orchestrator.execute(query)\n",
    "    \n",
    "    print(f\"✅ Quality Score: {response['quality_metrics']['overall_score']:.3f}\")\n",
    "    print(f\"🔄 Iterations Used: {response['process_metadata']['iterations_used']}\")\n",
    "    print(f\"🤖 Generation Method: {response['generation_method']}\")\n",
    "    print(f\"⏱️ Total Time: {response['process_metadata']['total_time']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n📝 Generated Response:\")\n",
    "    print(response['response'][:400] + \"...\" if len(response['response']) > 400 else response['response'])\n",
    "    \n",
    "    print(f\"\\n📚 Citations:\")\n",
    "    for citation in response['citations'][:3]:\n",
    "        print(f\"  • {citation}\")\n",
    "    \n",
    "    if idx < len(test_queries):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 PERFORMANCE COMPARISON:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Run performance analysis with the fixed system\n",
    "fixed_analyzer = PerformanceAnalyzer(orchestrator)\n",
    "fixed_comparison = fixed_analyzer.compare_with_traditional_rag(test_queries)\n",
    "\n",
    "print(f\" Agentic RAG Quality: {np.mean(fixed_comparison['agentic_quality']):.3f}\")\n",
    "print(f\" Agentic RAG Avg Iterations: {np.mean(fixed_comparison['agentic_iterations']):.1f}\")\n",
    "print(f\" Agentic RAG Avg Time: {np.mean(fixed_comparison['agentic_times']):.3f}s\")\n",
    "\n",
    "# Get system stats\n",
    "stats = orchestrator.get_system_stats()\n",
    "print(f\"\\n📈 System Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\n🎯 Expected Improvements:\")\n",
    "print(\"  ✅ Minimum 2 iterations guaranteed\")\n",
    "print(\"  ✅ Better quality scoring calibration\")\n",
    "print(\"  ✅ HuggingFace LLM integration (free)\")\n",
    "print(\"  ✅ Enhanced response generation\")\n",
    "print(\"  ✅ Improved strategy adaptation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9971e4c",
   "metadata": {},
   "source": [
    "## 7. Testing the Agentic RAG System <a id=\"testing1\"></a>\n",
    "\n",
    "Create comprehensive tests to ensure our system works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28cdc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting agentic RAG for query: What is the attention mechanism in transformers?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test Query 1: What is the attention mechanism in transformers?\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983dbefb9d484db8a8391ded8e6c48e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d182641c864afcbd2b145027aca0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4e1e5285ea4312815c1c8d3555f6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe1dc67ab664c43aaa08af6c27cbfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QualityAssessor: quality_score_0.56 - Assessed 5 results with score 0.56\n",
      "INFO:__main__:Iteration 1: Strategy=semantic, Quality=0.559\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "INFO:__main__:Orchestrator: completed_agentic_rag - Completed RAG with 1 iterations, final quality: 0.559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Type: factual\n",
      "Recommended Strategy: semantic\n",
      "Iterations Used: 1\n",
      "Total Time: 0.000 seconds\n",
      "Quality Score: 0.559\n",
      "\n",
      "Content Summary:\n",
      "it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. S...\n",
      "\n",
      "Citations:\n",
      "  [1] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "  [2] NIPS-2017-attention-is-all-you-need-Paper, page 9\n",
      "  [3] NIPS-2017-attention-is-all-you-need-Paper, page 5\n",
      "  [4] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "  [5] NIPS-2017-attention-is-all-you-need-Paper, page 5\n",
      "\n",
      "Top Retrieved Results:\n",
      "  1. Page 2 | Score: 0.555\n",
      "     Content: it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit ...\n",
      "  2. Page 9 | Score: 0.553\n",
      "     Content: We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involvi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting agentic RAG for query: How does multi-head attention work?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Complex queries require multi-modal retrieval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test Query 2: How does multi-head attention work?\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40299ee171e462fafdd0fdaaebec6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fc652e78fb497799ea444d461b4dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86126801c254c99886b35389f41443d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8885292608e4bb7a48485bddbaadb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QualityAssessor: quality_score_0.92 - Assessed 5 results with score 0.92\n",
      "INFO:__main__:Iteration 1: Strategy=hybrid, Quality=0.922\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "INFO:__main__:Orchestrator: completed_agentic_rag - Completed RAG with 1 iterations, final quality: 0.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Type: general\n",
      "Recommended Strategy: hybrid\n",
      "Iterations Used: 1\n",
      "Total Time: 0.000 seconds\n",
      "Quality Score: 0.922\n",
      "\n",
      "Content Summary:\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "output values. These are concatenated and once again projected, res...\n",
      "\n",
      "Citations:\n",
      "  [1] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "  [2] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "  [3] NIPS-2017-attention-is-all-you-need-Paper, page 7\n",
      "  [4] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "  [5] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "\n",
      "Top Retrieved Results:\n",
      "  1. Page 4 | Score: 1.524\n",
      "     Content: .\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial...\n",
      "  2. Page 4 | Score: 1.440\n",
      "     Content: Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "att...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting agentic RAG for query: Compare scaled dot-product attention with additive...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test Query 3: Compare scaled dot-product attention with additive attention\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfd587340a5467eb00e7fdf0e909078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f8f2edb2294b8caca46cdb512d7d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a59c02b2294d548e2402a4aa6bd81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89ba1192b994944914ea22718e8cd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QualityAssessor: quality_score_1.75 - Assessed 5 results with score 1.75\n",
      "INFO:__main__:Iteration 1: Strategy=hybrid, Quality=1.748\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "INFO:__main__:Orchestrator: completed_agentic_rag - Completed RAG with 1 iterations, final quality: 1.748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Type: comparison\n",
      "Recommended Strategy: hybrid\n",
      "Iterations Used: 1\n",
      "Total Time: 0.000 seconds\n",
      "Quality Score: 1.748\n",
      "\n",
      "Content Summary:\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices Kand V.\n",
      "\n",
      "Citations:\n",
      "  [1] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "  [2] NIPS-2017-attention-is-all-you-need-Paper, page 3\n",
      "  [3] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "  [4] NIPS-2017-attention-is-all-you-need-Paper, page 3\n",
      "  [5] NIPS-2017-attention-is-all-you-need-Paper, page 5\n",
      "\n",
      "Top Retrieved Results:\n",
      "  1. Page 4 | Score: 4.893\n",
      "     Content: Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "att...\n",
      "  2. Page 3 | Score: 3.770\n",
      "     Content: query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting agentic RAG for query: What are the computational complexities of differe...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test Query 4: What are the computational complexities of different attention mechanisms?\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd335f3db994dd882b44c010d538154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90e5a26f80841b5b3a8954ed73d6dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76405c53bedc4b158065ef178d85fe42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2127b8b7c9eb44d6822e09c4db3852e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QualityAssessor: quality_score_1.42 - Assessed 5 results with score 1.42\n",
      "INFO:__main__:Iteration 1: Strategy=hybrid, Quality=1.421\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "INFO:__main__:Orchestrator: completed_agentic_rag - Completed RAG with 1 iterations, final quality: 1.421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Type: general\n",
      "Recommended Strategy: hybrid\n",
      "Iterations Used: 1\n",
      "Total Time: 0.000 seconds\n",
      "Quality Score: 1.421\n",
      "\n",
      "Content Summary:\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations...\n",
      "\n",
      "Citations:\n",
      "  [1] NIPS-2017-attention-is-all-you-need-Paper, page 6\n",
      "  [2] NIPS-2017-attention-is-all-you-need-Paper, page 5\n",
      "  [3] NIPS-2017-attention-is-all-you-need-Paper, page 4\n",
      "  [4] NIPS-2017-attention-is-all-you-need-Paper, page 6\n",
      "  [5] NIPS-2017-attention-is-all-you-need-Paper, page 5\n",
      "\n",
      "Top Retrieved Results:\n",
      "  1. Page 6 | Score: 3.039\n",
      "     Content: different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, ...\n",
      "  2. Page 5 | Score: 2.868\n",
      "     Content: MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\n",
      "where headi = Attention(QWQ\n",
      "i ,KW K\n",
      "i ,VW V\n",
      "i )\n",
      "Where the projections are parameter matricesWQ\n",
      "i ∈Rdmod...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting agentic RAG for query: Explain the positional encoding in the transformer...\n",
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test Query 5: Explain the positional encoding in the transformer model\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c46d29adcf4d6fa3acf8f3f2340d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467beac272e847f1bb01b1affef60c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6388366212ce45d7b66368f0058bfb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b8faf4ed204c75af13180c16550fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QualityAssessor: quality_score_0.56 - Assessed 5 results with score 0.56\n",
      "INFO:__main__:Iteration 1: Strategy=semantic, Quality=0.559\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "INFO:__main__:Orchestrator: completed_agentic_rag - Completed RAG with 1 iterations, final quality: 0.559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Type: factual\n",
      "Recommended Strategy: semantic\n",
      "Iterations Used: 1\n",
      "Total Time: 0.000 seconds\n",
      "Quality Score: 0.559\n",
      "\n",
      "Content Summary:\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself.\n",
      "\n",
      "Citations:\n",
      "  [1] NIPS-2017-attention-is-all-you-need-Paper, page 3\n",
      "  [2] NIPS-2017-attention-is-all-you-need-Paper, page 5\n",
      "  [3] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "  [4] NIPS-2017-attention-is-all-you-need-Paper, page 2\n",
      "  [5] NIPS-2017-attention-is-all-you-need-Paper, page 5\n",
      "\n",
      "Top Retrieved Results:\n",
      "  1. Page 3 | Score: 0.550\n",
      "     Content: Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two...\n",
      "  2. Page 5 | Score: 0.530\n",
      "     Content: 3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we...\n",
      "\n",
      "============================================================\n",
      "SYSTEM PERFORMANCE STATISTICS\n",
      "============================================================\n",
      "Total Conversations: 10\n",
      "Average Iterations: 1.000\n",
      "Average Quality Score: 1.042\n",
      "Average Response Time: 0.810\n",
      "Retrieval Agent Calls: 10\n",
      "Quality Assessments: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Query {i+1}: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    response = orchestrator.execute(query)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nQuery Type: {orchestrator.query_analyzer.decision_history[-1].metadata['query_type']}\")\n",
    "    print(f\"Recommended Strategy: {orchestrator.query_analyzer.decision_history[-1].metadata['recommended_strategy']}\")\n",
    "    print(f\"Iterations Used: {response['process_metadata']['iterations_used']}\")\n",
    "    print(f\"Total Time: {response['process_metadata']['total_time']:.3f} seconds\")\n",
    "    print(f\"Quality Score: {response['quality_metrics']['overall_score']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nContent Summary:\")\n",
    "    print(response['content_summary'])\n",
    "    \n",
    "    print(f\"\\nCitations:\")\n",
    "    for citation in response['citations']:\n",
    "        print(f\"  {citation}\")\n",
    "    \n",
    "    print(f\"\\nTop Retrieved Results:\")\n",
    "    for j, result in enumerate(response['results'][:2]): \n",
    "        print(f\"  {j+1}. Page {result['page_number']} | Score: {result['score']:.3f}\")\n",
    "        print(f\"     Content: {result['content'][:150]}...\")\n",
    "    \n",
    "    if i < len(test_queries) - 1:  \n",
    "        time.sleep(1)  \n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SYSTEM PERFORMANCE STATISTICS\")\n",
    "print('='*60)\n",
    "\n",
    "stats = orchestrator.get_system_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b5651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/flan-t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: What is the attention mechanism in transformers?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model google/flan-t5-base loaded successfully!\n",
      "Testing Agentic RAG System\n",
      "==================================================\n",
      "\n",
      "Test 1: Query: What is the attention mechanism in transformers?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95b475efb364ecbae64fb2c9f76cce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.59 - Assessed 5 results with score 0.59\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=semantic, Quality=0.587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d374e392db7485ab8a347101a238a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.79 - Assessed 6 results with score 0.79\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.787\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_6_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.787\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: How does multi-head attention work?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Complex queries require multi-modal retrieval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Summary:\n",
      "it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. S...\n",
      "Quality Score: 0.7875\n",
      "Citations: ['[1] NIPS-2017-attention-is-all-you-need-Paper, page 2', '[2] NIPS-2017-attention-is-all-you-need-Paper, page 2', '[3] NIPS-2017-attention-is-all-you-need-Paper, page 2', '[4] NIPS-2017-attention-is-all-you-need-Paper, page 5', '[5] NIPS-2017-attention-is-all-you-need-Paper, page 8', '[6] NIPS-2017-attention-is-all-you-need-Paper, page 5']\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 2: Query: How does multi-head attention work?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433a31f661c84fa9800433095bc16d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.72 - Assessed 5 results with score 0.72\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de253e67171415c8935f7bba750aa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.71 - Assessed 6 results with score 0.71\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a021c8cbd731446db96ceb72605df0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_7_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.53 - Assessed 7 results with score 0.53\n",
      "INFO:__main__:🔄 Iteration 3: Strategy=semantic, Quality=0.526\n",
      "INFO:__main__:🛑 Stopping after 3 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 3 iterations, final quality: 0.724\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: Compare scaled dot-product attention with additive...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Summary:\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "output values. These are concatenated and once again projected, res...\n",
      "Quality Score: 0.7243560651358123\n",
      "Citations: ['[1] NIPS-2017-attention-is-all-you-need-Paper, page 4', '[2] NIPS-2017-attention-is-all-you-need-Paper, page 4', '[3] NIPS-2017-attention-is-all-you-need-Paper, page 7', '[4] NIPS-2017-attention-is-all-you-need-Paper, page 2', '[5] NIPS-2017-attention-is-all-you-need-Paper, page 2']\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 3: Query: Compare scaled dot-product attention with additive attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d36aa9a8dbd40b1ba123d82145095e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.86 - Assessed 5 results with score 0.86\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89946ff64ada4c9aa3f220e6b6465cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.86 - Assessed 6 results with score 0.86\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.858\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.865\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: What are the computational complexities of differe...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Summary:\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices Kand V.\n",
      "Quality Score: 0.865\n",
      "Citations: ['[1] NIPS-2017-attention-is-all-you-need-Paper, page 4', '[2] NIPS-2017-attention-is-all-you-need-Paper, page 3', '[3] NIPS-2017-attention-is-all-you-need-Paper, page 4', '[4] NIPS-2017-attention-is-all-you-need-Paper, page 3', '[5] NIPS-2017-attention-is-all-you-need-Paper, page 5']\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 4: Query: What are the computational complexities of different attention mechanisms?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c953322005394c1796c85c1e48aba88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.79 - Assessed 5 results with score 0.79\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234ab2ce482347c6b10f8dc686def846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.78 - Assessed 6 results with score 0.78\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.780\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.795\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: Explain the positional encoding in the transformer...\n",
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Summary:\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations...\n",
      "Quality Score: 0.7949999999999999\n",
      "Citations: ['[1] NIPS-2017-attention-is-all-you-need-Paper, page 6', '[2] NIPS-2017-attention-is-all-you-need-Paper, page 5', '[3] NIPS-2017-attention-is-all-you-need-Paper, page 4', '[4] NIPS-2017-attention-is-all-you-need-Paper, page 6', '[5] NIPS-2017-attention-is-all-you-need-Paper, page 5']\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 5: Query: Explain the positional encoding in the transformer model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56585c6ee0d9473e88f6a6f892584229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.57 - Assessed 5 results with score 0.57\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=semantic, Quality=0.572\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35502f0d6b904c36a40ea86e0b9ffa86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.81 - Assessed 6 results with score 0.81\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.810\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_6_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Summary:\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "5\n",
      "\n",
      "learned and ﬁxed [8].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i) = sin(pos/100002i/dmodel )\n",
      "PE(pos,2i+1) = cos(pos/100002i/dmodel )...\n",
      "Quality Score: 0.8097222222222222\n",
      "Citations: ['[1] NIPS-2017-attention-is-all-you-need-Paper, page 5', '[2] NIPS-2017-attention-is-all-you-need-Paper, page 6', '[3] NIPS-2017-attention-is-all-you-need-Paper, page 9', '[4] NIPS-2017-attention-is-all-you-need-Paper, page 5', '[5] NIPS-2017-attention-is-all-you-need-Paper, page 6', '[6] NIPS-2017-attention-is-all-you-need-Paper, page 7']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "orchestrator = OrchestrationAgent(retriever)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the attention mechanism in transformers?\",\n",
    "    \"How does multi-head attention work?\",\n",
    "    \"Compare scaled dot-product attention with additive attention\",\n",
    "    \"What are the computational complexities of different attention mechanisms?\",\n",
    "    \"Explain the positional encoding in the transformer model\"\n",
    "]\n",
    "\n",
    "print(\"Testing Agentic RAG System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for idx, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nTest {idx}: Query: {query}\")\n",
    "    response = orchestrator.execute(query)\n",
    "    print(\"Response Summary:\")\n",
    "    print(response['content_summary'])\n",
    "    print(\"Quality Score:\", response['quality_metrics']['overall_score'])\n",
    "    print(\"Citations:\", response['citations'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f117dd",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis and Evaluation <a id=\"analysis\"></a>\n",
    "\n",
    "Let's analyze the performance of our Agentic RAG system and compare it with traditional approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea4317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\omen\\anaconda3\\lib\\site-packages (4.56.2)🔧 Installing transformers if needed...\n",
      "✅ Transformers already installed\n",
      "\n",
      "============================================================\n",
      "🧪 STARTING COMPREHENSIVE FIXED TESTS\n",
      "============================================================\n",
      "\n",
      "🔍 MANUAL VERIFICATION OF AGENTIC BEHAVIOR\n",
      "==================================================\n",
      "\n",
      "1️⃣ Traditional RAG:\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\omen\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\omen\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "   ---------------------------------------- 0.0/374.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/374.9 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/374.9 kB 660.6 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 71.7/374.9 kB 653.6 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 102.4/374.9 kB 845.5 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 102.4/374.9 kB 845.5 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 204.8/374.9 kB 831.5 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 286.7/374.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 348.2/374.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 348.2/374.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 348.2/374.9 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 374.9/374.9 kB 805.0 kB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadb0152b2cd4cfc8be132bb4c3915e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: What is the attention mechanism in transformers?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Results: 5\n",
      "   Time: 0.055s\n",
      "   Top result score: 0.555\n",
      "\n",
      "2️⃣ Fixed Agentic RAG:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f32d23b63504280a058e86d84c1a28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.59 - Assessed 5 results with score 0.59\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=semantic, Quality=0.587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681af58bdc0140a0b9b3cf667a2b0134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.79 - Assessed 6 results with score 0.79\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.787\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_6_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iterations: 2\n",
      "   Quality Score: 0.787\n",
      "   Time: 2.691s\n",
      "   Generation Method: hf_generated\n",
      "   Strategies Tried: ['semantic', 'hybrid']\n",
      "\n",
      "📊 COMPARISON:\n",
      "   Quality Improvement: +0.233\n",
      "   Time Overhead: +2.636s\n",
      "   Iterations Used: 2\n",
      "🚀 RUNNING FIXED AGENTIC RAG COMPREHENSIVE TEST\n",
      "============================================================\n",
      "🔬 Running Comprehensive RAG Comparison...\n",
      "============================================================\n",
      "\n",
      "📝 Testing Query 1: What is the attention mechanism in transformers?...\n",
      "   🔄 Running Traditional RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8248a3f1534667b33e8185b359af4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: What is the attention mechanism in transformers?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ⏱️ Time: 0.032s, Quality: 0.648\n",
      "   🤖 Running Agentic RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445d0250166040c3b8c8f3723afedb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.59 - Assessed 5 results with score 0.59\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=semantic, Quality=0.587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b31d7b728b3499abf4b201c27b1350b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.79 - Assessed 6 results with score 0.79\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.787\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_6_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ⏱️ Time: 2.410s, Quality: 0.787, Iterations: 2\n",
      "      📊 Quality Improvement: +0.139\n",
      "\n",
      "📝 Testing Query 2: How does multi-head attention work?...\n",
      "   🔄 Running Traditional RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb953d79565481bb72e7d8834fcef09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: How does multi-head attention work?...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Complex queries require multi-modal retrieval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ⏱️ Time: 0.034s, Quality: 0.702\n",
      "   🤖 Running Agentic RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f0fb54746c4a5da33c0538d4c37b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.72 - Assessed 5 results with score 0.72\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600ddc37f72f4037b52863baf88f543f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.71 - Assessed 6 results with score 0.71\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77587d01dc7e4af69c1c530300c43cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_7_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.53 - Assessed 7 results with score 0.53\n",
      "INFO:__main__:🔄 Iteration 3: Strategy=semantic, Quality=0.526\n",
      "INFO:__main__:🛑 Stopping after 3 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 3 iterations, final quality: 0.724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ⏱️ Time: 2.322s, Quality: 0.724, Iterations: 3\n",
      "      📊 Quality Improvement: +0.023\n",
      "\n",
      "📝 Testing Query 3: Compare scaled dot-product attention with additive...\n",
      "   🔄 Running Traditional RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c3a07c32d940218d848f47deb3332c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: Compare scaled dot-product attention with additive...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ⏱️ Time: 0.034s, Quality: 0.756\n",
      "   🤖 Running Agentic RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cee73ab410b4fe785207645b14cc124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.86 - Assessed 5 results with score 0.86\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1941df02ed6c41c5881c8f9425be036f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.86 - Assessed 6 results with score 0.86\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.858\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ⏱️ Time: 12.216s, Quality: 0.865, Iterations: 2\n",
      "      📊 Quality Improvement: +0.109\n",
      "\n",
      "📝 Testing Query 4: What are the key innovations in transformer archit...\n",
      "   🔄 Running Traditional RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3481509fad624dda82b6ce04264b985d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: What are the key innovations in transformer archit...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Complex queries require multi-modal retrieval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ⏱️ Time: 0.033s, Quality: 0.568\n",
      "   🤖 Running Agentic RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b6471757034b5d9cfafb2aaf987f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.77 - Assessed 5 results with score 0.77\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f58755b72b64f9cbf942fa06303f0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.77 - Assessed 6 results with score 0.77\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.771\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_6_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ⏱️ Time: 4.047s, Quality: 0.771, Iterations: 2\n",
      "      📊 Quality Improvement: +0.203\n",
      "\n",
      "============================================================\n",
      "🏆 FINAL PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "📊 QUALITY METRICS:\n",
      "   Traditional RAG:  0.668\n",
      "   Agentic RAG:      0.787\n",
      "   Improvement:      +0.118 (+17.7%)\n",
      "   🎯 STATUS: ✅ AGENTIC RAG WINS!\n",
      "\n",
      "⏱️ TIME METRICS:\n",
      "   Traditional RAG:  0.033s\n",
      "   Agentic RAG:      5.249s\n",
      "   Overhead:         +5.215s\n",
      "\n",
      "🔄 AGENTIC BEHAVIOR:\n",
      "   Average Iterations: 2.2\n",
      "   Quality Consistency: 0.730\n",
      "\n",
      "🎯 OVERALL ASSESSMENT:\n",
      "   🏆 Agentic RAG provides significant quality improvement!\n",
      "\n",
      "============================================================\n",
      "✅ ALL TESTS COMPLETED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "\n",
    "class PerformanceAnalyzer:\n",
    "    def __init__(self, agentic_orchestrator, traditional_retriever):\n",
    "        self.agentic_orchestrator = agentic_orchestrator\n",
    "        self.traditional_retriever = traditional_retriever\n",
    "        self.results_cache = {}\n",
    "    \n",
    "    def run_comprehensive_comparison(self, test_queries: List[str]) -> Dict[str, Any]:\n",
    "        print(\"🔬 Running Comprehensive RAG Comparison...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {\n",
    "            'traditional': {'times': [], 'quality_scores': [], 'result_counts': []},\n",
    "            'agentic': {'times': [], 'quality_scores': [], 'result_counts': [], 'iterations': []},\n",
    "            'improvements': {}\n",
    "        }\n",
    "        \n",
    "        for i, query in enumerate(test_queries, 1):\n",
    "            print(f\"\\n📝 Testing Query {i}: {query[:50]}...\")\n",
    "            print(\"   🔄 Running Traditional RAG...\")\n",
    "            trad_start = time.time()\n",
    "            trad_results = self.traditional_retriever.semantic_search(query, k=5)\n",
    "            trad_time = time.time() - trad_start\n",
    "            trad_quality = self._assess_quality_simple(query, trad_results) \n",
    "            results['traditional']['times'].append(trad_time)\n",
    "            results['traditional']['quality_scores'].append(trad_quality)\n",
    "            results['traditional']['result_counts'].append(len(trad_results))\n",
    "            print(f\"      ⏱️ Time: {trad_time:.3f}s, Quality: {trad_quality:.3f}\")\n",
    "            print(\"   🤖 Running Agentic RAG...\")\n",
    "            agent_start = time.time()\n",
    "            agent_response = self.agentic_orchestrator.execute(query)\n",
    "            agent_time = time.time() - agent_start\n",
    "            \n",
    "            agent_quality = agent_response['quality_metrics']['overall_score']\n",
    "            agent_iterations = agent_response['process_metadata']['iterations_used']\n",
    "            \n",
    "            results['agentic']['times'].append(agent_time)\n",
    "            results['agentic']['quality_scores'].append(agent_quality)\n",
    "            results['agentic']['result_counts'].append(agent_response['quality_metrics']['result_count'])\n",
    "            results['agentic']['iterations'].append(agent_iterations)\n",
    "            \n",
    "            print(f\"      ⏱️ Time: {agent_time:.3f}s, Quality: {agent_quality:.3f}, Iterations: {agent_iterations}\")\n",
    "\n",
    "            quality_improvement = agent_quality - trad_quality\n",
    "            print(f\"      📊 Quality Improvement: {quality_improvement:+.3f}\")\n",
    "\n",
    "        results['improvements'] = self._calculate_improvements(results)\n",
    "        self._print_final_comparison(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _assess_quality_simple(self, query: str, results: List) -> float:\n",
    "        if not results:\n",
    "            return 0.0\n",
    "        \n",
    "        avg_score = np.mean([r.score for r in results]) if results else 0.0\n",
    "        result_bonus = min(len(results) / 5.0, 1.0)  \n",
    "        normalized_score = min(avg_score, 1.0) if avg_score > 0 else 0.0\n",
    "        \n",
    "        return (normalized_score * 0.7) + (result_bonus * 0.3)\n",
    "    \n",
    "    def _calculate_improvements(self, results: Dict) -> Dict:\n",
    "        trad_avg_quality = np.mean(results['traditional']['quality_scores'])\n",
    "        agent_avg_quality = np.mean(results['agentic']['quality_scores'])\n",
    "        \n",
    "        trad_avg_time = np.mean(results['traditional']['times'])\n",
    "        agent_avg_time = np.mean(results['agentic']['times'])\n",
    "        \n",
    "        return {\n",
    "            'quality_improvement': agent_avg_quality - trad_avg_quality,\n",
    "            'quality_improvement_pct': ((agent_avg_quality - trad_avg_quality) / trad_avg_quality * 100) if trad_avg_quality > 0 else 0,\n",
    "            'time_overhead': agent_avg_time - trad_avg_time,\n",
    "            'avg_iterations': np.mean(results['agentic']['iterations']),\n",
    "            'quality_consistency': np.std(results['agentic']['quality_scores']) / np.std(results['traditional']['quality_scores']) if np.std(results['traditional']['quality_scores']) > 0 else 1.0\n",
    "        }\n",
    "    \n",
    "    def _print_final_comparison(self, results: Dict):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"🏆 FINAL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        trad_avg_quality = np.mean(results['traditional']['quality_scores'])\n",
    "        agent_avg_quality = np.mean(results['agentic']['quality_scores'])\n",
    "        quality_improvement = results['improvements']['quality_improvement']\n",
    "        \n",
    "        print(f\"\\n📊 QUALITY METRICS:\")\n",
    "        print(f\"   Traditional RAG:  {trad_avg_quality:.3f}\")\n",
    "        print(f\"   Agentic RAG:      {agent_avg_quality:.3f}\")\n",
    "        print(f\"   Improvement:      {quality_improvement:+.3f} ({results['improvements']['quality_improvement_pct']:+.1f}%)\")\n",
    "        \n",
    "        if quality_improvement > 0:\n",
    "            print(\"   🎯 STATUS: ✅ AGENTIC RAG WINS!\")\n",
    "        else:\n",
    "            print(\"   ⚠️  STATUS: ❌ TRADITIONAL RAG BETTER\")\n",
    "  \n",
    "        trad_avg_time = np.mean(results['traditional']['times'])\n",
    "        agent_avg_time = np.mean(results['agentic']['times'])\n",
    "        \n",
    "        print(f\"\\n⏱️ TIME METRICS:\")\n",
    "        print(f\"   Traditional RAG:  {trad_avg_time:.3f}s\")\n",
    "        print(f\"   Agentic RAG:      {agent_avg_time:.3f}s\")\n",
    "        print(f\"   Overhead:         +{results['improvements']['time_overhead']:.3f}s\")\n",
    "        print(f\"\\n🔄 AGENTIC BEHAVIOR:\")\n",
    "        print(f\"   Average Iterations: {results['improvements']['avg_iterations']:.1f}\")\n",
    "        print(f\"   Quality Consistency: {results['improvements']['quality_consistency']:.3f}\")\n",
    "        print(f\"\\n🎯 OVERALL ASSESSMENT:\")\n",
    "        if quality_improvement > 0.05:\n",
    "            print(\"   🏆 Agentic RAG provides significant quality improvement!\")\n",
    "        elif quality_improvement > 0.01:\n",
    "            print(\"   ✅ Agentic RAG provides modest quality improvement\")\n",
    "        else:\n",
    "            print(\"   ❌ Agentic RAG needs optimization - no quality improvement\")\n",
    "\n",
    "def run_fixed_comprehensive_test():\n",
    "    print(\"🚀 RUNNING AGENTIC RAG COMPREHENSIVE TEST\")\n",
    "    print(\"=\"*60)\n",
    "    if 'fixed_orchestrator' not in globals():\n",
    "        print(\"⚠️ Creating Fixed Orchestrator...\")\n",
    "        fixed_orchestrator = OrchestrationAgent(retriever, use_hf_llm=True)\n",
    "    else:\n",
    "        fixed_orchestrator = globals()['fixed_orchestrator']\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is the attention mechanism in transformers?\",\n",
    "        \"How does multi-head attention work?\",\n",
    "        \"Compare scaled dot-product attention with additive attention\",\n",
    "        \"What are the key innovations in transformer architecture?\"\n",
    "    ]\n",
    "    \n",
    "    analyzer = PerformanceAnalyzer(fixed_orchestrator, retriever)\n",
    "    results = analyzer.run_comprehensive_comparison(test_queries)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def manual_verification_test():\n",
    "    print(\"\\n🔍 MANUAL VERIFICATION OF AGENTIC BEHAVIOR\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_query = \"What is the attention mechanism in transformers?\"\n",
    "\n",
    "    print(\"\\n1️⃣ Traditional RAG:\")\n",
    "    trad_start = time.time()\n",
    "    trad_results = retriever.semantic_search(test_query, k=5)\n",
    "    trad_time = time.time() - trad_start\n",
    "    print(f\"   Results: {len(trad_results)}\")\n",
    "    print(f\"   Time: {trad_time:.3f}s\")\n",
    "    print(f\"   Top result score: {trad_results[0].score:.3f}\" if trad_results else \"No results\")\n",
    "\n",
    "    print(\"\\n2️⃣ Agentic RAG:\")\n",
    "    if 'fixed_orchestrator' not in globals():\n",
    "        fixed_orchestrator = OrchestrationAgent(retriever, use_hf_llm=True)\n",
    "    else:\n",
    "        fixed_orchestrator = globals()['fixed_orchestrator']\n",
    "    \n",
    "    agent_start = time.time()\n",
    "    agent_response = fixed_orchestrator.execute(test_query)\n",
    "    agent_time = time.time() - agent_start\n",
    "    \n",
    "    print(f\"   Iterations: {agent_response['process_metadata']['iterations_used']}\")\n",
    "    print(f\"   Quality Score: {agent_response['quality_metrics']['overall_score']:.3f}\")\n",
    "    print(f\"   Time: {agent_time:.3f}s\")\n",
    "    print(f\"   Generation Method: {agent_response['generation_method']}\")\n",
    "    print(f\"   Strategies Tried: {agent_response['process_metadata']['strategies_tried']}\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(f\"\\n📊 COMPARISON:\")\n",
    "    quality_diff = agent_response['quality_metrics']['overall_score'] - (trad_results[0].score if trad_results else 0)\n",
    "    print(f\"   Quality Improvement: {quality_diff:+.3f}\")\n",
    "    print(f\"   Time Overhead: +{agent_time - trad_time:.3f}s\")\n",
    "    print(f\"   Iterations Used: {agent_response['process_metadata']['iterations_used']}\")\n",
    "    \n",
    "    return {\n",
    "        'traditional': {'time': trad_time, 'results': len(trad_results)},\n",
    "        'agentic': {\n",
    "            'time': agent_time, \n",
    "            'quality': agent_response['quality_metrics']['overall_score'],\n",
    "            'iterations': agent_response['process_metadata']['iterations_used']\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"🔧 Installing transformers if needed...\")\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"✅ Transformers already installed\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing transformers...\")\n",
    "    !pip install transformers torch\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🧪 STARTING COMPREHENSIVE FIXED TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "manual_results = manual_verification_test()\n",
    "\n",
    "comprehensive_results = run_fixed_comprehensive_test()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✅ ALL TESTS COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bafd41",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Test Framework <a id=\"testing2\"></a>\n",
    "\n",
    "Let's create a robust testing framework to ensure our Agentic RAG system works correctly across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b13673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Complex queries require multi-modal retrieval\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Complex queries require multi-modal retrieval\n",
      "INFO:__main__:QueryAnalyzer: recommend_lexical_retrieval - Specific queries need exact term matching\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Complex queries require multi-modal retrieval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Comprehensive Test Suite\n",
      "==================================================\n",
      "\n",
      "1. Testing Query Analysis Agent...\n",
      "   Query Type Accuracy: 100.00%\n",
      "   Strategy Recommendation Accuracy: 100.00%\n",
      "\n",
      "2. Testing Retrieval Quality...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f35161f32d4a28ae4e5a9713562ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.60 - Assessed 5 results with score 0.60\n",
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used lexical strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.76 - Assessed 5 results with score 0.76\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6323dc2d8f0f4a69ae8f864beb43c3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.76 - Assessed 5 results with score 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Semantic Strategy - Quality: 0.598\n",
      "   Lexical Strategy - Quality: 0.757\n",
      "   Hybrid Strategy - Quality: 0.757\n",
      "\n",
      "3. Testing Citation Generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccec01ccf7094b1ca91774aebcf61383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_3_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:CitationAgent: generated_3_citations - Created proper citations for all retrieved sources\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: What are the key innovations in the attention mech...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Citations Generated: 3\n",
      "   Bibliography Entries: 1\n",
      "   Has Page Numbers: True\n",
      "\n",
      "4. Testing Iterative Improvement...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4686bf7dcec46adad4271c34223e0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.80 - Assessed 5 results with score 0.80\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc443699eb314733821ad67a77a29728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.79 - Assessed 6 results with score 0.79\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.787\n",
      "INFO:__main__:🛑 Stopping after 2 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 2 iterations, final quality: 0.800\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: ...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iterations Used: 2\n",
      "   Quality Improved: False\n",
      "   Final Quality: 0.800\n",
      "\n",
      "5. Testing Edge Cases...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ddc2a8c4d34551ad0018a717698de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.28 - Assessed 5 results with score 0.28\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dbd63fe1084ade8ce05217be5caf19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.26 - Assessed 6 results with score 0.26\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.264\n",
      "INFO:__main__:RetrievalAgent: retrieved_0_documents - Used lexical strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: low_quality_assessment - No results to evaluate\n",
      "INFO:__main__:🔄 Iteration 3: Strategy=lexical, Quality=0.000\n",
      "INFO:__main__:🛑 Stopping after 3 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 3 iterations, final quality: 0.277\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: a...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1c9804de474cb09d3d6adb742d2765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.45 - Assessed 5 results with score 0.45\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb377310309748d6afdb75bb11f74a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.46 - Assessed 6 results with score 0.46\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.460\n",
      "INFO:__main__:RetrievalAgent: retrieved_7_documents - Used lexical strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.62 - Assessed 7 results with score 0.62\n",
      "INFO:__main__:🔄 Iteration 3: Strategy=lexical, Quality=0.618\n",
      "INFO:__main__:🛑 Stopping after 3 iterations\n",
      "INFO:__main__:CitationAgent: generated_7_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 3 iterations, final quality: 0.618\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: What is the meaning of life, the universe, and eve...\n",
      "INFO:__main__:QueryAnalyzer: recommend_semantic_retrieval - Factual queries benefit from semantic understanding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eb7135c3af4381bb489a329958f51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.33 - Assessed 5 results with score 0.33\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=semantic, Quality=0.325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83825f2bc184f218dfa97d98f628755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.69 - Assessed 6 results with score 0.69\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.689\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187528d4ca2479eb16f9d3f1e398506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_7_documents - Used semantic strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.37 - Assessed 7 results with score 0.37\n",
      "INFO:__main__:🔄 Iteration 3: Strategy=semantic, Quality=0.375\n",
      "INFO:__main__:🛑 Stopping after 3 iterations\n",
      "INFO:__main__:CitationAgent: generated_6_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 3 iterations, final quality: 0.689\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: xyz123 qwerty asdfgh...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582c9b81b76947cf8dff5f413c2b8ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.29 - Assessed 5 results with score 0.29\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.293\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd3d051f1fe4bfeb96c7556c10d498c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.29 - Assessed 6 results with score 0.29\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.295\n",
      "INFO:__main__:RetrievalAgent: retrieved_0_documents - Used lexical strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: low_quality_assessment - No results to evaluate\n",
      "INFO:__main__:🔄 Iteration 3: Strategy=lexical, Quality=0.000\n",
      "INFO:__main__:🛑 Stopping after 3 iterations\n",
      "INFO:__main__:CitationAgent: generated_6_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 3 iterations, final quality: 0.295\n",
      "INFO:__main__:🚀 Starting FIXED agentic RAG for query: 🤖 🧠 💻...\n",
      "INFO:__main__:QueryAnalyzer: recommend_hybrid_retrieval - Default to hybrid approach for balanced results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835e6a3c70ad49cc920047181f2091db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_5_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.26 - Assessed 5 results with score 0.26\n",
      "INFO:__main__:🔄 Iteration 1: Strategy=hybrid, Quality=0.259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b349b862debd4d52b4d3621e674724f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RetrievalAgent: retrieved_6_documents - Used hybrid strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: quality_score_0.26 - Assessed 6 results with score 0.26\n",
      "INFO:__main__:🔄 Iteration 2: Strategy=hybrid, Quality=0.259\n",
      "INFO:__main__:RetrievalAgent: retrieved_0_documents - Used lexical strategy to find relevant documents\n",
      "INFO:__main__:QualityAssessor: low_quality_assessment - No results to evaluate\n",
      "INFO:__main__:🔄 Iteration 3: Strategy=lexical, Quality=0.000\n",
      "INFO:__main__:🛑 Stopping after 3 iterations\n",
      "INFO:__main__:CitationAgent: generated_5_citations - Created proper citations for all retrieved sources\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:__main__:HF_LLMGenerator: generated_response - Generated response using HF model approach\n",
      "INFO:__main__:Orchestrator: completed_fixed_agentic_rag - ✅ Completed FIXED RAG with 3 iterations, final quality: 0.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Success Rate: 100.00%\n",
      "\n",
      "==================================================\n",
      "Test Suite Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "class AgenticRAGTestSuite:\n",
    "    def __init__(self, orchestrator: OrchestrationAgent):\n",
    "        self.orchestrator = orchestrator\n",
    "        self.test_results = []\n",
    "    \n",
    "    def test_query_analysis_agent(self):\n",
    "        \"\"\"Test the Query Analysis Agent\"\"\"\n",
    "        test_cases = [\n",
    "            (\"What is machine learning?\", \"factual\", \"semantic\"),\n",
    "            (\"Compare supervised and unsupervised learning\", \"comparison\", \"hybrid\"),\n",
    "            (\"How to implement a neural network?\", \"procedural\", \"hybrid\"),\n",
    "            (\"When was the transformer invented?\", \"specific\", \"lexical\"),\n",
    "            (\"Why does attention work better than RNNs for long sequences?\", \"analytical\", \"hybrid\")\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for query, expected_type, expected_strategy in test_cases:\n",
    "            decision = self.orchestrator.query_analyzer.execute(query)\n",
    "            actual_type = decision.metadata['query_type']\n",
    "            actual_strategy = decision.metadata['recommended_strategy']\n",
    "            \n",
    "            type_correct = actual_type == expected_type\n",
    "            strategy_correct = actual_strategy == expected_strategy\n",
    "            \n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'expected_type': expected_type,\n",
    "                'actual_type': actual_type,\n",
    "                'expected_strategy': expected_strategy,\n",
    "                'actual_strategy': actual_strategy,\n",
    "                'type_correct': type_correct,\n",
    "                'strategy_correct': strategy_correct,\n",
    "                'confidence': decision.confidence\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_retrieval_quality(self):\n",
    "        test_query = \"What is the attention mechanism in neural networks?\"\n",
    "        \n",
    "        strategies = ['semantic', 'lexical', 'hybrid']\n",
    "        results = {}\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            retrieval_results = self.orchestrator.retrieval_agent.execute(test_query, strategy, k=5)\n",
    "            quality_assessment = self.orchestrator.quality_assessor.execute(test_query, retrieval_results)\n",
    "            \n",
    "            results[strategy] = {\n",
    "                'results_count': len(retrieval_results),\n",
    "                'quality_score': quality_assessment['quality_score'],\n",
    "                'issues': quality_assessment['issues'],\n",
    "                'metrics': quality_assessment['metrics']\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_citation_generation(self):\n",
    "        test_query = \"Explain the transformer architecture\"\n",
    "        results = self.orchestrator.retrieval_agent.execute(test_query, 'hybrid', k=3)\n",
    "        citations = self.orchestrator.citation_agent.execute(results)\n",
    "        \n",
    "        test_results = {\n",
    "            'citation_count': len(citations['citations']),\n",
    "            'bibliography_count': len(citations['bibliography']),\n",
    "            'has_page_numbers': any('page' in citation for citation in citations['citations']),\n",
    "            'unique_sources': len(set(frozenset(source.items()) for source in citations['source_map'].values())),\n",
    "            'citations': citations['citations']\n",
    "        }\n",
    "        \n",
    "        return test_results\n",
    "    \n",
    "    def test_iterative_improvement(self):\n",
    "        test_query = \"What are the key innovations in the attention mechanism?\"\n",
    "        response = self.orchestrator.execute(test_query)\n",
    "        \n",
    "        iterations = response['process_metadata']['strategies_tried']\n",
    "        conversation = self.orchestrator.get_conversation_history()[-1]\n",
    "        \n",
    "        quality_progression = [iteration['quality_score'] for iteration in conversation['iterations']]\n",
    "        \n",
    "        return {\n",
    "            'iterations_used': len(iterations),\n",
    "            'strategies_tried': iterations,\n",
    "            'quality_progression': quality_progression,\n",
    "            'final_quality': response['quality_metrics']['overall_score'],\n",
    "            'improved': len(quality_progression) > 1 and quality_progression[-1] >= quality_progression[0]\n",
    "        }\n",
    "    \n",
    "    def test_edge_cases(self):\n",
    "        edge_cases = [\n",
    "            (\"\", \"empty_query\"),\n",
    "            (\"a\", \"very_short_query\"),\n",
    "            (\"What is the meaning of life, the universe, and everything, and how does it relate to artificial intelligence, machine learning, deep learning, neural networks, transformers, attention mechanisms, and the future of humanity?\", \"very_long_query\"),\n",
    "            (\"xyz123 qwerty asdfgh\", \"nonsense_query\"),\n",
    "            (\"🤖 🧠 💻\", \"emoji_query\")\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for query, case_type in edge_cases:\n",
    "            try:\n",
    "                response = self.orchestrator.execute(query)\n",
    "                results.append({\n",
    "                    'case_type': case_type,\n",
    "                    'query': query,\n",
    "                    'success': True,\n",
    "                    'quality_score': response['quality_metrics']['overall_score'],\n",
    "                    'results_count': response['quality_metrics']['result_count'],\n",
    "                    'error': None\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'case_type': case_type,\n",
    "                    'query': query,\n",
    "                    'success': False,\n",
    "                    'quality_score': 0,\n",
    "                    'results_count': 0,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        print(\"Running Comprehensive Test Suite\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Test 1: Query Analysis\n",
    "        print(\"\\n1. Testing Query Analysis Agent...\")\n",
    "        query_analysis_results = self.test_query_analysis_agent()\n",
    "        type_accuracy = sum(r['type_correct'] for r in query_analysis_results) / len(query_analysis_results)\n",
    "        strategy_accuracy = sum(r['strategy_correct'] for r in query_analysis_results) / len(query_analysis_results)\n",
    "        \n",
    "        print(f\"   Query Type Accuracy: {type_accuracy:.2%}\")\n",
    "        print(f\"   Strategy Recommendation Accuracy: {strategy_accuracy:.2%}\")\n",
    "        \n",
    "        # Test 2: Retrieval Quality\n",
    "        print(\"\\n2. Testing Retrieval Quality...\")\n",
    "        retrieval_results = self.test_retrieval_quality()\n",
    "        for strategy, metrics in retrieval_results.items():\n",
    "            print(f\"   {strategy.title()} Strategy - Quality: {metrics['quality_score']:.3f}\")\n",
    "        \n",
    "        # Test 3: Citation Generation\n",
    "        print(\"\\n3. Testing Citation Generation...\")\n",
    "        citation_results = self.test_citation_generation()\n",
    "        print(f\"   Citations Generated: {citation_results['citation_count']}\")\n",
    "        print(f\"   Bibliography Entries: {citation_results['bibliography_count']}\")\n",
    "        print(f\"   Has Page Numbers: {citation_results['has_page_numbers']}\")\n",
    "        \n",
    "        # Test 4: Iterative Improvement\n",
    "        print(\"\\n4. Testing Iterative Improvement...\")\n",
    "        improvement_results = self.test_iterative_improvement()\n",
    "        print(f\"   Iterations Used: {improvement_results['iterations_used']}\")\n",
    "        print(f\"   Quality Improved: {improvement_results['improved']}\")\n",
    "        print(f\"   Final Quality: {improvement_results['final_quality']:.3f}\")\n",
    "        \n",
    "        # Test 5: Edge Cases\n",
    "        print(\"\\n5. Testing Edge Cases...\")\n",
    "        edge_case_results = self.test_edge_cases()\n",
    "        success_rate = sum(r['success'] for r in edge_case_results) / len(edge_case_results)\n",
    "        print(f\"   Success Rate: {success_rate:.2%}\")\n",
    "        \n",
    "        for result in edge_case_results:\n",
    "            if not result['success']:\n",
    "                print(f\"   Failed: {result['case_type']} - {result['error']}\")\n",
    "        \n",
    "        # Compile final test report\n",
    "        self.test_results = {\n",
    "            'query_analysis': {\n",
    "                'type_accuracy': type_accuracy,\n",
    "                'strategy_accuracy': strategy_accuracy,\n",
    "                'details': query_analysis_results\n",
    "            },\n",
    "            'retrieval_quality': retrieval_results,\n",
    "            'citation_generation': citation_results,\n",
    "            'iterative_improvement': improvement_results,\n",
    "            'edge_cases': {\n",
    "                'success_rate': success_rate,\n",
    "                'details': edge_case_results\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"Test Suite Completed Successfully!\")\n",
    "        return self.test_results\n",
    "\n",
    "# Run the comprehensive test suite\n",
    "test_suite = AgenticRAGTestSuite(orchestrator)\n",
    "test_results = test_suite.run_all_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
